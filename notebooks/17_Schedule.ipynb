{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "915c11da-f47a-40fb-9665-bae6bf979a80",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_GPU_ALLOCATOR'] = 'cuda_malloc_async'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7aeb4e95-7f7f-4bf2-b920-1ff62709bd56",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-26 00:33:37.192098: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-12-26 00:33:37.218397: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-12-26 00:33:37.225207: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-12-26 00:33:37.242512: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-12-26 00:33:38.820221: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "from typing import List, Dict, Any\n",
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21983b17-da7a-4754-a8db-75e86d7b4210",
   "metadata": {},
   "source": [
    "## Types of Learning Schedules"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11742f98-5e26-42d5-b4ae-7669d2cc7794",
   "metadata": {},
   "source": [
    "### Cosine Warmup Learning Rate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "454945b6-c8e4-440c-a9b1-9db2873f7b77",
   "metadata": {},
   "source": [
    "$\n",
    "\\eta_t = \n",
    "\\begin{cases} \n",
    "\\eta_{max} \\cdot \\left(\\frac{t}{t_{warmup}}\\right) & \\text{if } t < t_{warmup} \\\\\n",
    "\\eta_{min} + \\frac{1}{2}(\\eta_{max} - \\eta_{min})\\left(1 + \\cos\\left(\\pi \\cdot \\frac{t - t_{warmup}}{t_{max} - t_{warmup}}\\right)\\right) & \\text{otherwise} \n",
    "\\end{cases}\n",
    "$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b605da60-0084-41c7-b398-1cd9719fb6f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CosineWarmupSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    def __init__(self, base_learning_rate: float, minimum_learning_rate: float, warmup_steps: int, total_steps:int):\n",
    "        super(CosineWarmupSchedule, self).__init__()\n",
    "        \n",
    "        self.base_learning_rate = tf.constant(base_learning_rate, dtype= tf.float32)\n",
    "        self.minimum_learning_rate = tf.constant(minimum_learning_rate, dtype= tf.float32)\n",
    "        self.warmup_steps = tf.constant(warmup_steps, dtype= tf.int64)\n",
    "        self.total_steps = tf.constant(total_steps, dtype= tf.int64)\n",
    "        self.pi = tf.constant(3.141592653589793, tf.float32)\n",
    "        \n",
    "    def __call__(self, step: tf.Tensor):\n",
    "        \n",
    "        step = tf.cast(step, dtype = tf.float32)\n",
    "\n",
    "        warmup_steps = tf.cast(self.warmup_steps, dtype = tf.float32)\n",
    "        total_steps = tf.cast(self.total_steps, dtype = tf.float32)\n",
    "\n",
    "        # Phase 1: Warm up learning rate\n",
    "        warmup_learning_rate = self.base_learning_rate * tf.minimum(1.0, step / tf.maximum(1.0, warmup_steps))\n",
    "\n",
    "        # Phase 2: Cosine Decay\n",
    "        diff_in_steps = tf.maximum(1.0,total_steps - warmup_steps)\n",
    "        progress = tf.clip_by_value((step - warmup_steps)/diff_in_steps, 0.0, 1.0)\n",
    "        cosine_decay = self.minimum_learning_rate + 0.5 * (self.base_learning_rate - self.minimum_learning_rate) * (1 + tf.math.cos(self.pi * progress))\n",
    "\n",
    "        return tf.where(step < warmup_steps, warmup_learning_rate, cosine_decay)       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b04c87f5-19f9-493b-9936-4f76102a7960",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1766727220.610575    2529 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "I0000 00:00:1766727220.715037    2529 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "I0000 00:00:1766727220.715105    2529 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "I0000 00:00:1766727220.716817    2529 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "I0000 00:00:1766727220.716894    2529 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "I0000 00:00:1766727220.716930    2529 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "I0000 00:00:1766727220.940676    2529 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "I0000 00:00:1766727220.940844    2529 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2025-12-26 00:33:40.940857: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2112] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "2025-12-26 00:33:40.940884: I tensorflow/core/common_runtime/gpu/gpu_process_state.cc:198] Using CUDA malloc Async allocator for GPU: 0\n",
      "I0000 00:00:1766727220.943186    2529 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2025-12-26 00:33:40.943228: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2021] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 9558 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4070, pci bus id: 0000:01:00.0, compute capability: 8.9\n"
     ]
    }
   ],
   "source": [
    "cosine_decay = CosineWarmupSchedule(base_learning_rate = 0.01, minimum_learning_rate = 0.0001, warmup_steps = 1000, total_steps = 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0b9b4bfc-86c7-4537-91dc-d4f951d9eab8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=1e-04>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_decay(10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbb16c9c-8f5a-4f4f-bfc8-78b981bb07fc",
   "metadata": {},
   "source": [
    "### Constant Learning Rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c99f01c0-e551-4818-be0c-bf335165145e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConstantLearningSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    def __init__(self, base_learning_rate: float):\n",
    "        super(ConstantLearningSchedule, self).__init__()\n",
    "        \n",
    "        self.base_learning_rate = tf.constant(base_learning_rate, dtype= tf.float32)\n",
    "        \n",
    "    def __call__(self, step: tf.Tensor):\n",
    "        return self.base_learning_rate      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c260e547-bd77-4e7a-8fe3-d0afddb12c3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "constant_lr = ConstantLearningSchedule(base_learning_rate = 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "72d15a74-f18b-457f-bbad-c36c618bacf1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=0.01>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "constant_lr(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdb46be8-7e26-49d9-b202-1e7c6df325eb",
   "metadata": {},
   "source": [
    "### Step Decay Learning Rate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bab236d8-e8d2-4be3-aee3-cf6f412a5979",
   "metadata": {},
   "source": [
    "$\n",
    "\\text{lr}=\\text{lr}_{\\text{initial}}\\times \\gamma^{n(t)}\n",
    "$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a3f29b97-9696-410e-b9be-fdb73cd72c17",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StepDecaySchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    def __init__(self, base_learning_rate: float, gamma: float, milestones: list[int]):\n",
    "        super(StepDecaySchedule, self).__init__()\n",
    "        \n",
    "        self.base_learning_rate = tf.constant(base_learning_rate, dtype= tf.float32)\n",
    "        self.gamma = tf.constant(gamma, dtype= tf.float32)\n",
    "        self.milestones = tf.constant(milestones, dtype= tf.int64)\n",
    "        \n",
    "    def __call__(self, step: tf.Tensor):\n",
    "        \n",
    "        step = tf.cast(step, dtype = tf.int64)\n",
    "\n",
    "        # Calculating the number of milestones passed\n",
    "        num_milestones = tf.reduce_sum(tf.cast(step >= self.milestones, dtype= tf.float32))\n",
    "\n",
    "        # Calculating the learning rate\n",
    "        learning_rate = self.base_learning_rate * tf.math.pow(self.gamma, num_milestones)\n",
    "\n",
    "        return learning_rate             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "328db1f2-8e56-4c87-88b6-1a147903300d",
   "metadata": {},
   "outputs": [],
   "source": [
    "step_decay = StepDecaySchedule(base_learning_rate = 0.01, gamma = 0.1, milestones = [10000, 20000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b1d1d5c3-b890-4545-addf-68bbf1090bd5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=1e-04>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "step_decay(20000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6da3edd6-f38f-46da-a60c-7d159c7fe619",
   "metadata": {},
   "source": [
    "### Exponential Decay Learning Rate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23bfe216-159f-4e9a-b88b-7cefd09fa6bf",
   "metadata": {},
   "source": [
    "$\n",
    "\\text{lr}=\\text{lr}_{\\text{initial}}\\times \\gamma^{\\frac{t}{k}}\n",
    "$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "69bb6f53-77c9-4ade-8ada-fad584d7c82b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExponentialDecaySchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    def __init__(self, base_learning_rate: float, gamma: float, decay_interval: int):\n",
    "        super(ExponentialDecaySchedule, self).__init__()\n",
    "        \n",
    "        self.base_learning_rate = tf.constant(base_learning_rate, dtype= tf.float32)\n",
    "        self.gamma = tf.constant(gamma, dtype= tf.float32)\n",
    "        self.decay_interval = tf.constant(decay_interval, dtype= tf.int64)\n",
    "        \n",
    "    def __call__(self, step: tf.Tensor):\n",
    "        \n",
    "        step = tf.cast(step, dtype = tf.int64)\n",
    "\n",
    "        # Calculating the decay factor\n",
    "        decay_factor = tf.math.pow(self.gamma, tf.cast(step/self.decay_interval, dtype = tf.float32))\n",
    "\n",
    "        # Calculating the exponential decay\n",
    "        learning_rate = self.base_learning_rate * decay_factor\n",
    "\n",
    "        return learning_rate          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "39e85b32-819b-4fcb-933d-c3e3823e7614",
   "metadata": {},
   "outputs": [],
   "source": [
    "exponential_decay = ExponentialDecaySchedule(base_learning_rate = 0.01, gamma = 0.9, decay_interval = 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e4a3c61d-bd43-4106-9636-90dbccbb4f98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=0.0034867835>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exponential_decay(10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d82b7f2f-44cb-47e1-bdf4-47e38a7ef9a8",
   "metadata": {},
   "source": [
    "## Learning Rate Schedule Factory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8f45fd2c-cdcc-43f4-b072-f7c11ef51819",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LearningRateScheduleFactory:\n",
    "    @staticmethod\n",
    "    def build(config: dict[str, Any]):\n",
    "        lr_schedule_config = LearningRateScheduleFactory._create_scheduler_config_from_main_config(config)\n",
    "\n",
    "        lr_schedule = LearningRateScheduleFactory._build_learning_schedule(lr_schedule_config)\n",
    "\n",
    "        return lr_schedule\n",
    "        \n",
    "    @staticmethod\n",
    "    def _create_scheduler_config_from_main_config(config: dict[str,Any]):\n",
    "        scheduler = config['train'].get('scheduler')\n",
    "        warmup_config = scheduler.get('warmup', {})\n",
    "        step_config = scheduler.get('step', {})\n",
    "        multistep_config = scheduler.get('multistep', {})\n",
    "    \n",
    "        schedule_config = {\n",
    "            'learning_schedule': scheduler.get('name',\"constant\"),\n",
    "            'base_learning_rate': scheduler.get('base_lr', 0.1),\n",
    "            'mininum_learning_rate': scheduler.get('min_lr', 0.001),\n",
    "            'total_steps': scheduler.get('total_steps', None),\n",
    "            'warmup_epochs': warmup_config.get('epochs',10),\n",
    "            'warmup_steps': warmup_config.get('steps', 15),\n",
    "            'warmup_enabled': warmup_config.get('enabled',True),\n",
    "            'warmup_start_factor': warmup_config.get('start_factor',0.1),\n",
    "            'warmup_end_factor': warmup_config.get('end_factor',0.1),\n",
    "            'warmup_mode': warmup_config.get('mode',0.1),\n",
    "            'step_drop_every_epochs': step_config.get('drop_every_epochs', None),\n",
    "            'step_gamma': step_config.get('gamma', None),\n",
    "            'multistep_milestones': multistep_config.get('milestones_epochs', [])\n",
    "        }\n",
    "        \n",
    "        return schedule_config\n",
    "    \n",
    "    @staticmethod\n",
    "    def _build_learning_schedule(config: dict[str,Any]):\n",
    "        # Need to select the correct learning schedule\n",
    "        if config['learning_schedule'] == 'cosine_warmup':\n",
    "            learning_schedule = CosineWarmupSchedule(base_learning_rate = config['base_learning_rate'], minimum_learning_rate = config['mininum_learning_rate'], warmup_steps = config['warmup_steps'], total_steps = config['total_steps'])\n",
    "        elif config['learning_schedule'] == 'step_decay':\n",
    "            learning_schedule = StepDecaySchedule(base_learning_rate = config['base_learning_rate'], gamma = config['step_gamma'], milestones = config['multistep_milestones'])\n",
    "        elif config['learning_schedule'] == 'exponential_decay':\n",
    "            learning_schedule = ExponentialDecaySchedule(base_learning_rate = config['base_learning_rate'], gamma = config['step_gamma'], decay_interval = config['step_drop_every_epochs'])\n",
    "        elif config['learning_schedule'] == 'constant':\n",
    "            learning_schedule = ConstantLearningSchedule(base_learning_rate = config['base_learning_rate'])\n",
    "        else:\n",
    "            raise ValueError(f\"Invalid Learning Rate Schedule: {config['learning_schedule']}\")\n",
    "\n",
    "        return learning_schedule"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b6af275-2b5b-436e-84b2-176014426fa9",
   "metadata": {},
   "source": [
    "## Factory Pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "96da6691-3df9-4185-bf91-4af927a5284a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mobilenetv2ssd.core.config import load_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6d518ab7-5f0b-4d45-8478-4d3d8a83c4d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "main_cfg_path = \"configs/train/default.yaml\"\n",
    "model_cfg_path = \"configs/model/mobilenetv2_ssd_voc.yaml\"\n",
    "data_cfg_path = \"configs/data/voc_224.yaml\"\n",
    "eval_cfg_path = \"configs/eval/default.yaml\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "786a0144-e578-47ef-8871-2c9a8a830c6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = load_config(main_cfg_path,model_cfg_path,data_cfg_path,eval_cfg_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fe614df3-43b8-4bf7-938c-0330a5762346",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'interval': 'step',\n",
       " 'name': 'cosine_warmup',\n",
       " 'base_lr': 0.001,\n",
       " 'min_lr': 1e-05,\n",
       " 'total_steps': 10000,\n",
       " 'warmup': {'enabled': True,\n",
       "  'epochs': 5,\n",
       "  'steps': 10,\n",
       "  'start_factor': 0.1,\n",
       "  'end_factor': 1.0,\n",
       "  'mode': 'linear'},\n",
       " 'step': {'drop_every_epochs': 10, 'gamma': 0.1},\n",
       " 'multistep': {'milestones_epochs': [30, 45], 'gamma': 0.1}}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config['train']['scheduler']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "37a5937b-ae96-4339-9690-8e93d5a88049",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LearningRateScheduleFactory.build(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5d2a62e2-3d08-4a9d-a708-2d12f75ad78e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.CosineWarmupSchedule at 0x71c54ab019f0>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23c1e479-43ec-426a-be68-fbaba618d35b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
