{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "db4c0f66-d9f6-4aed-b359-3df5db914fa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import hashlib, json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "7cef8757-d732-4be6-8420-4a52e7f5ddd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mobilenetv2ssd.core.config import load_config\n",
    "from mobilenetv2ssd.models.ssd.ops.anchor_ops_tf import build_priors, build_priors_batched"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92b8a48f-0f31-4606-9a94-737b421929e1",
   "metadata": {},
   "source": [
    "## Config Files To Test Orchestration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "a05f6506-1d92-451b-92ab-349934e759d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "main_cfg_path = \"configs/train/default.yaml\"\n",
    "model_cfg_path = \"configs/model/mobilenetv2_ssd_voc.yaml\"\n",
    "data_cfg_path = \"configs/data/voc_224.yaml\"\n",
    "eval_cfg_path = \"configs/eval/default.yaml\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "400d4006-d92e-4d60-8b45-909150ccedd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = load_config(main_cfg_path,model_cfg_path,data_cfg_path,eval_cfg_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "cb780a3e-e5eb-47a5-a8e4-aad131dd73d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dataset_split': 'val',\n",
       " 'nms': {'iou_threshold': 0.5,\n",
       "  'score_threshold': 0.05,\n",
       "  'max_detections_per_image': 100,\n",
       "  'max_detections_per_class': 50},\n",
       " 'metrics': {'type': 'voc_ap',\n",
       "  'iou_thresholds': [0.5],\n",
       "  'use_07_metric': False},\n",
       " 'visualization': {'enabled': False,\n",
       "  'max_images': 16,\n",
       "  'output_dir': 'eval_vis'}}"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config['eval']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "692115bd-68bd-4e3f-8bcd-51710fdb890d",
   "metadata": {},
   "source": [
    "## Helper Functions For Orchestration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "51a9f6f4-7451-4d80-83a8-f2dfd74e4e88",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _extract_information_from_model_config(model_config : dict[str, any]):\n",
    "    config = model_config['model']['priors']\n",
    "    prior_config = {\n",
    "        # Big prior hyperparameters\n",
    "        \"image_size\": config['image_size'],\n",
    "        \"strides\": config['strides'],\n",
    "        \"feature_map_shapes\": None if 'feature_map_shapes' not in config else config['feature_map_shapes'],\n",
    "        \n",
    "        # Prior Shape Determinants\n",
    "        \"min_scale\": config['min_scale'],\n",
    "        \"max_scale\": config['max_scale'],\n",
    "        \"scales\": None if 'scales' not in config else config['scales'],\n",
    "        \"aspect_ratios\": config['aspect_ratios'],\n",
    "\n",
    "        # Extra options that can be added in the model\n",
    "        \"two_scales_per_octave\": True, # Saw this in a article about RetinaNet and just added the option for later iterations\n",
    "        \"extra_scales_per_layer\": True,\n",
    "        \"format\": \"cxcywh\",\n",
    "        \"normalize\": True, # Always assumes normalization but can be added in future iterations for more control\n",
    "        \"clip\": True,\n",
    "        \"dtype\": \"float32\", # Important for later since I will be using this to shrink the computation on embedded hardware\n",
    "\n",
    "        # Tilting\n",
    "        \"center_offset\" : 0.5,\n",
    "        \"align_corners\" : False,\n",
    "    }\n",
    "\n",
    "    return prior_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "85bbac84-111b-48f4-9f68-5fc72b8f2e08",
   "metadata": {},
   "outputs": [],
   "source": [
    "prior_config = _extract_information_from_model_config(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "78292f2a-d7e0-46d5-b74e-fcfe0ef932f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'image_size': [224, 224],\n",
       " 'strides': [8, 16, 32, 64, 128, 224],\n",
       " 'feature_map_shapes': None,\n",
       " 'min_scale': 0.2,\n",
       " 'max_scale': 0.95,\n",
       " 'scales': None,\n",
       " 'aspect_ratios': [[1.0, 2.0, 0.5],\n",
       "  [1.0, 2.0, 0.5, 3.0, 0.3333],\n",
       "  [1.0, 2.0, 0.5, 3.0, 0.3333],\n",
       "  [1.0, 2.0, 0.5],\n",
       "  [1.0, 2.0, 0.5],\n",
       "  [1.0, 2.0, 0.5]],\n",
       " 'two_scales_per_octave': True,\n",
       " 'extra_scales_per_layer': True,\n",
       " 'format': 'cxcywh',\n",
       " 'normalize': True,\n",
       " 'clip': True,\n",
       " 'dtype': 'float32',\n",
       " 'center_offset': 0.5,\n",
       " 'align_corners': False}"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prior_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "3e0f3a90-1ca6-4eb1-8a36-382060cd0637",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _compute_prior_config_fingerprint(config):\n",
    "    serialized = json.dumps(config, sort_keys=True).encode()\n",
    "    return hashlib.md5(serialized).hexdigest()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "e9184337-7ff5-48d7-b53f-8bed36e05105",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'9b5713d9a9273fd9a182d3d74d0ce1d1'"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_compute_prior_config_fingerprint(prior_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "a3d32df3-492e-479e-abdf-82e2905700ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _validate_prior_config(config):\n",
    "    # Checking if the format of the config is correct\n",
    "    image_size = config.get(\"image_size\")\n",
    "    if not isinstance(image_size, (list, tuple)) or len(image_size) != 2:\n",
    "        raise ValueError(\"image_size must be a sequence of length 2 (H, W).\")\n",
    "\n",
    "    h, w = image_size\n",
    "    if not (isinstance(h, (int, float)) and isinstance(w, (int, float))):\n",
    "        raise ValueError(\"image_size values must be numeric.\")\n",
    "    if not (h > 0 and w > 0):\n",
    "        raise ValueError(\"image_size must be positive in both dimensions.\")\n",
    "\n",
    "    # Checking if the length of the strides and the aspect_ratios is the same\n",
    "    strides = config.get(\"strides\")\n",
    "    fm_shapes = config.get(\"feature_map_shapes\")\n",
    "\n",
    "    if strides is None and fm_shapes is None:\n",
    "        raise ValueError(\"At least one of 'strides' or 'feature_map_shapes' must be provided.\")\n",
    "\n",
    "    num_levels = None\n",
    "\n",
    "    if strides is not None:\n",
    "        if not isinstance(strides, (list, tuple)) or len(strides) == 0:\n",
    "            raise ValueError(\"'strides' must be a non-empty list when provided.\")\n",
    "        for s in strides:\n",
    "            if not (isinstance(s, int) and s > 0):\n",
    "                raise ValueError(\"All strides must be positive integers.\")\n",
    "        num_levels = len(strides)\n",
    "\n",
    "    if fm_shapes is not None:\n",
    "        if not isinstance(fm_shapes, (list, tuple)) or len(fm_shapes) == 0:\n",
    "            raise ValueError(\"'feature_map_shapes' must be a non-empty list when provided.\")\n",
    "        for shape in fm_shapes:\n",
    "            if not isinstance(shape, (list, tuple)) or len(shape) != 2:\n",
    "                raise ValueError(\n",
    "                    \"Each entry in 'feature_map_shapes' must be a (h, w) pair.\"\n",
    "                )\n",
    "            h_l, w_l = shape\n",
    "            if not (isinstance(h_l, int) and isinstance(w_l, int)):\n",
    "                raise ValueError(\n",
    "                    \"Entries in 'feature_map_shapes' must be integer (h, w) pairs.\"\n",
    "                )\n",
    "            if not (h_l > 0 and w_l > 0):\n",
    "                raise ValueError(\n",
    "                    \"Each feature map shape (h, w) must be positive.\"\n",
    "                )\n",
    "\n",
    "        if num_levels is None:\n",
    "            num_levels = len(fm_shapes)\n",
    "        else:\n",
    "            if len(fm_shapes) != num_levels:\n",
    "                raise ValueError(\n",
    "                    \"Length of 'feature_map_shapes' must match length of 'strides' \"\n",
    "                    f\"({len(fm_shapes)} vs {num_levels}).\"\n",
    "                )\n",
    "\n",
    "    if num_levels is None:\n",
    "        raise ValueError(\"Internal error: num_levels could not be inferred from config.\")\n",
    "\n",
    "    aspect_ratios = config.get(\"aspect_ratios\")\n",
    "\n",
    "    if aspect_ratios is not None:\n",
    "        if not isinstance(aspect_ratios, (list, tuple)) or len(aspect_ratios) == 0:\n",
    "            raise ValueError(\"'aspect_ratios' must be a non-empty list when provided.\")\n",
    "\n",
    "        first_ar = aspect_ratios[0]\n",
    "\n",
    "        # Helper inline checks (no inner functions)\n",
    "        if isinstance(first_ar, (int, float)):\n",
    "            # 1D: broadcast later inside standardize_aspect_ratios\n",
    "            for ar in aspect_ratios:\n",
    "                if not isinstance(ar, (int, float)):\n",
    "                    raise ValueError(\"All aspect ratio values must be numeric.\")\n",
    "                if not (ar > 0):\n",
    "                    raise ValueError(\"All aspect ratio values must be positive.\")\n",
    "        else:\n",
    "            # 2D: per level\n",
    "            if len(aspect_ratios) != num_levels:\n",
    "                raise ValueError(\n",
    "                    \"Length of 'aspect_ratios' (per-level) must match num_levels \"\n",
    "                    f\"({len(aspect_ratios)} vs {num_levels}).\"\n",
    "                )\n",
    "            for lvl_idx, lvl_ars in enumerate(aspect_ratios):\n",
    "                if not isinstance(lvl_ars, (list, tuple)) or len(lvl_ars) == 0:\n",
    "                    raise ValueError(\n",
    "                        f\"'aspect_ratios[{lvl_idx}]' must be a non-empty list of numbers.\"\n",
    "                    )\n",
    "                for ar in lvl_ars:\n",
    "                    if not isinstance(ar, (int, float)):\n",
    "                        raise ValueError(\n",
    "                            f\"All aspect ratios in 'aspect_ratios[{lvl_idx}]' \"\n",
    "                            \"must be numeric.\"\n",
    "                        )\n",
    "                    if not (ar > 0):\n",
    "                        raise ValueError(\n",
    "                            f\"All aspect ratios in 'aspect_ratios[{lvl_idx}]' \"\n",
    "                            \"must be positive.\"\n",
    "                        )\n",
    "            \n",
    "\n",
    "    scales = config.get(\"scales\")\n",
    "    min_scale = config.get(\"min_scale\")\n",
    "    max_scale = config.get(\"max_scale\")\n",
    "\n",
    "    if scales is not None:\n",
    "        if not isinstance(scales, (list, tuple)) or len(scales) == 0:\n",
    "            raise ValueError(\"'scales' must be a non-empty list when provided.\")\n",
    "\n",
    "        first_scale = scales[0]\n",
    "\n",
    "        if isinstance(first_scale, (int, float)):\n",
    "            # 1D list of scales\n",
    "            for s in scales:\n",
    "                if not isinstance(s, (int, float)):\n",
    "                    raise ValueError(\"All scale values in 'scales' must be numeric.\")\n",
    "                if not (0 < s <= 1):\n",
    "                    raise ValueError(\n",
    "                        f\"Scale value {s} in 'scales' is out of range; \"\n",
    "                        \"must satisfy 0 < s <= 1.\"\n",
    "                    )\n",
    "        else:\n",
    "            # 2D: per level\n",
    "            if len(scales) != num_levels:\n",
    "                raise ValueError(\n",
    "                    \"Length of 'scales' (per-level) must match num_levels \"\n",
    "                    f\"({len(scales)} vs {num_levels}).\"\n",
    "                )\n",
    "            for lvl_idx, lvl_scales in enumerate(scales):\n",
    "                if not isinstance(lvl_scales, (list, tuple)) or len(lvl_scales) == 0:\n",
    "                    raise ValueError(\n",
    "                        f\"'scales[{lvl_idx}]' must be a non-empty list of numbers.\"\n",
    "                    )\n",
    "                for s in lvl_scales:\n",
    "                    if not isinstance(s, (int, float)):\n",
    "                        raise ValueError(\n",
    "                            f\"All scale values in 'scales[{lvl_idx}]' must be numeric.\"\n",
    "                        )\n",
    "                    if not (0 < s <= 1):\n",
    "                        raise ValueError(\n",
    "                            f\"Scale value {s} in 'scales[{lvl_idx}]' is out of range; \"\n",
    "                            \"must satisfy 0 < s <= 1.\"\n",
    "                        )\n",
    "\n",
    "        # If explicit scales are provided, min_scale/max_scale are optional.\n",
    "        # You can optionally add extra consistency checks here if you want.\n",
    "    else:\n",
    "        # No explicit scales â†’ we must have valid min_scale + max_scale\n",
    "        if min_scale is None or max_scale is None:\n",
    "            raise ValueError(\n",
    "                \"When 'scales' is None, both 'min_scale' and 'max_scale' must be provided.\"\n",
    "            )\n",
    "        if not isinstance(min_scale, (int, float)) or not isinstance(max_scale, (int, float)):\n",
    "            raise ValueError(\"'min_scale' and 'max_scale' must be numeric when provided.\")\n",
    "        if not (0 < min_scale <= max_scale <= 1):\n",
    "            raise ValueError(\n",
    "                \"The relationship 0 < min_scale <= max_scale <= 1 must hold \"\n",
    "                f\"(got min_scale={min_scale}, max_scale={max_scale}).\"\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "f59b30dc-b45f-4db3-b5c6-dcf784949a37",
   "metadata": {},
   "outputs": [],
   "source": [
    "_validate_prior_config(prior_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "424d9771-bb24-47d2-b9f1-05406d74fd49",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _convert_dtype_to_tf(dtype: str):\n",
    "    dtype_converter = {\n",
    "        'int32': tf.int32,\n",
    "        'int16': tf.int16,\n",
    "        'int64': tf.int64,\n",
    "        'int8': tf.int8,\n",
    "        'float16': tf.float16,\n",
    "        'float32': tf.float32,\n",
    "        'float64': tf.float64,        \n",
    "    }\n",
    "\n",
    "    return dtype_converter.get(dtype, tf.float16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "f97f7990-1d94-47f8-b2c9-7b59723794c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tf.float32"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_convert_dtype_to_tf(\"float32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "6129095a-ce3e-4829-a844-0c3aad9e4e23",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _cache_priors(fingerprint: str, priors, meta: dict):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "af833c2f-9431-45e2-a70f-fe7a64835c45",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_cached_priors(fingerprint: str):\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22c325e2-7928-4a70-9a8d-2bd44111be99",
   "metadata": {},
   "source": [
    "## Orchestration Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "bbb34392-7a24-4da4-9db7-cbc3655b0bd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_priors_from_config(model_config,batch_size: int| None = None, evaluation_config = None):\n",
    "    # The function should be doing very simple steps on top of the operations\n",
    "    # Steps:\n",
    "    # 1. Extract the configuration used to create the priors from model_config\n",
    "    # 2. Compute a config hash (Later implement a small cache system to reduce computations)\n",
    "    # 3. Validate if the config is correct\n",
    "    # 3. Computer the priors for one image\n",
    "    # 4. Batch those priors to be used for all the images (Kept as is and then the model refines it using deltas)\n",
    "    prior_config = _extract_information_from_model_config(model_config)\n",
    "    prior_config['fingerprint'] = _compute_prior_config_fingerprint(prior_config)\n",
    "    _validate_prior_config(prior_config)\n",
    "    # Check if the config exists in the model\n",
    "    cached = _get_cached_priors(prior_config['fingerprint'])\n",
    "    if cached is not None:\n",
    "        priors, meta = cached\n",
    "    else:\n",
    "        priors,meta = build_priors(image_size = prior_config['image_size'], strides = prior_config['strides'], feature_map_shapes = prior_config['feature_map_shapes'],scales = prior_config['scales'],aspect_ratios = prior_config['aspect_ratios'],s_min = prior_config['min_scale'],s_max = prior_config['max_scale'],include_extra = prior_config['extra_scales_per_layer'],clip = prior_config['clip'],dtype= _convert_dtype_to_tf(prior_config['dtype']))\n",
    "        # Cache Priors\n",
    "        _cache_priors(meta['fingerprint'],priors,meta)\n",
    "\n",
    "    if batch_size is not None:\n",
    "        priors = build_priors_batched(priors,batch_size)\n",
    "\n",
    "    return priors, meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "19505217-a08b-4d5f-832a-9d9fbbf0c06a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(7280, 4), dtype=float32, numpy=\n",
       " array([[0.01785714, 0.01785714, 0.2       , 0.2       ],\n",
       "        [0.01785714, 0.01785714, 0.26457512, 0.26457512],\n",
       "        [0.01785714, 0.01785714, 0.28284273, 0.14142136],\n",
       "        ...,\n",
       "        [0.5       , 0.5       , 1.        , 0.6892024 ],\n",
       "        [0.5       , 0.5       , 0.67175144, 1.        ],\n",
       "        [0.5       , 0.5       , 0.6892024 , 1.        ]], dtype=float32)>,\n",
       " {'image_size': (224, 224),\n",
       "  'feature_map_sizes': [(28, 28), (14, 14), (7, 7), (4, 4), (2, 2), (1, 1)],\n",
       "  'strides': [8, 16, 32, 64, 128, 224],\n",
       "  'scales_per_layer': [[0.2, 0.264575131106459],\n",
       "   [0.35, 0.4183300132670378],\n",
       "   [0.5, 0.5700877125495689],\n",
       "   [0.6499999999999999, 0.7211102550927978],\n",
       "   [0.8, 0.8717797887081347],\n",
       "   [0.95, 0.9746794344808963]],\n",
       "  'ratios_per_layer': [[1, 2.0, 0.5],\n",
       "   [1, 2.0, 3.0, 0.5, 0.3333],\n",
       "   [1, 2.0, 3.0, 0.5, 0.3333],\n",
       "   [1, 2.0, 0.5],\n",
       "   [1, 2.0, 0.5],\n",
       "   [1, 2.0, 0.5]],\n",
       "  'number_of_anchors_per_layer': <tf.Tensor: shape=(6,), dtype=int32, numpy=array([4704, 1960,  490,   96,   24,    6], dtype=int32)>,\n",
       "  'cells_per_layer': <tf.Tensor: shape=(6,), dtype=int32, numpy=array([784, 196,  49,  16,   4,   1], dtype=int32)>,\n",
       "  'anchors_per_cell': <tf.Tensor: shape=(6,), dtype=int32, numpy=array([ 6, 10, 10,  6,  6,  6], dtype=int32)>,\n",
       "  'total_number_of_anchors': <tf.Tensor: shape=(), dtype=int32, numpy=7280>,\n",
       "  'fingerprint': '0642d186a4941f18bfe6f7f58edc2ed9'})"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "build_priors_from_config(config)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
