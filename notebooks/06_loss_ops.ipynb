{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "41cf600e-1cd2-4075-9a59-6aa63fb414c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from typing import Iterable, Literal, overload\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import cv2 as cv\n",
    "import numpy as np\n",
    "import math\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "458d654d-d9a1-4612-bf5f-203a1a12e611",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['TF_GPU_ALLOCATOR'] = 'cuda_malloc_async'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b971b695-7e42-475f-8fe2-bacef04dbfd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-29 19:17:16.605163: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-11-29 19:17:16.648585: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-11-29 19:17:16.661830: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-11-29 19:17:16.729591: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-11-29 19:17:18.171440: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b3c18f4-8e6d-4b37-a15f-575b99e8cdea",
   "metadata": {},
   "source": [
    "## Implementing the Building Blocks For Loss Calculations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a045d50-65bc-407d-818e-68902aa1c6b7",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Localization Losses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b82d4ea2-7643-4fe8-a7ba-00cef1a5411b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Smooth L1 Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4938a151-9f68-47f9-b329-ecd8c09900c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def smooth_l1_loss(predicted_values, target, beta, reduction = \"sum\"):\n",
    "\n",
    "    # Calculate the difference between the two values\n",
    "    difference = predicted_values - target\n",
    "    absolute_difference = tf.math.abs(difference)\n",
    "\n",
    "    # Masking which values require L1 and L2\n",
    "    small_mask = absolute_difference < beta\n",
    "    large_mask = tf.logical_not(small_mask)\n",
    "    \n",
    "    # Calculate where the formula needs to change\n",
    "    errors = tf.where(small_mask, 0.5*(difference**2)/beta,  difference)\n",
    "    errors = tf.where(large_mask, absolute_difference - (0.5*beta),errors)\n",
    "\n",
    "    # Sum over the four coordinates\n",
    "    errors = tf.reduce_sum(errors,axis=-1)\n",
    "\n",
    "    # Reduction strategy\n",
    "    if reduction == \"sum\":\n",
    "        loss = tf.reduce_sum(errors)\n",
    "    elif reduction == \"max\":\n",
    "        loss = tf.reduce_max(errors)\n",
    "    elif reduction == \"mean\":\n",
    "        loss = tf.reduce_mean(errors)\n",
    "    else:\n",
    "        loss = errors\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5d1809cb-92f5-467f-9ac2-20f24eb417d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1764461839.480739   72728 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "I0000 00:00:1764461839.582107   72728 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "I0000 00:00:1764461839.582170   72728 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "I0000 00:00:1764461839.584047   72728 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "I0000 00:00:1764461839.584102   72728 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "I0000 00:00:1764461839.584136   72728 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "I0000 00:00:1764461839.773530   72728 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "I0000 00:00:1764461839.773611   72728 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2025-11-29 19:17:19.773622: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2112] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "2025-11-29 19:17:19.773666: I tensorflow/core/common_runtime/gpu/gpu_process_state.cc:198] Using CUDA malloc Async allocator for GPU: 0\n",
      "I0000 00:00:1764461839.774076   72728 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2025-11-29 19:17:19.774147: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2021] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 9558 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4070, pci bus id: 0000:01:00.0, compute capability: 8.9\n"
     ]
    }
   ],
   "source": [
    "pred = tf.constant([\n",
    "    [0.0,  0.0,  0.0,  0.0],   # box 0\n",
    "    [0.2, -0.4,  1.2, -2.0],   # box 1\n",
    "], dtype=tf.float32)\n",
    "target = tf.constant([\n",
    "    [0.0,  0.0,  0.0,  0.0],   # box 0\n",
    "    [0.0,  0.0,  0.0,  0.0],   # box 1\n",
    "], dtype=tf.float32)\n",
    "beta = 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1e9f0948-4122-4c33-a3d2-a016ce5225e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=2.3>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "smooth_l1_loss(pred,target,beta, reduction=\"sum\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7a6ddcdf-6601-472b-b341-31f0e2d01b74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2,), dtype=float32, numpy=array([0. , 2.3], dtype=float32)>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "smooth_l1_loss(pred,target,beta, reduction=\"none\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "928c2000-353b-49c0-9b2e-0e9e35f35b0e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### L1 Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d830e6c8-fd90-4d6a-a0a1-f1cd91fdaa64",
   "metadata": {},
   "outputs": [],
   "source": [
    "def l1_loss(predicted_values,target,reduction = \"sum\"):\n",
    "\n",
    "    # Calculate the difference between the pred and the actual values\n",
    "    difference = predicted_values - target\n",
    "    absolute_difference = tf.math.abs(difference)\n",
    "\n",
    "    # Sum over the four coordinates\n",
    "    errors = tf.reduce_sum(absolute_difference,axis=-1)\n",
    "\n",
    "    # Reduction strategy\n",
    "    if reduction == \"sum\":\n",
    "        loss = tf.reduce_sum(errors)\n",
    "    elif reduction == \"max\":\n",
    "        loss = tf.reduce_max(errors)\n",
    "    elif reduction == \"mean\":\n",
    "        loss = tf.reduce_mean(errors)\n",
    "    else:\n",
    "        loss = errors\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e39c0756-3129-4897-9649-9a133468de14",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = tf.constant([\n",
    "    [0.0,  0.0,  0.0,  0.0],\n",
    "    [0.2, -0.4,  1.2, -2.0],\n",
    "], dtype=tf.float32)\n",
    "\n",
    "target = tf.zeros_like(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "847b0569-7814-4c81-b354-36f17463b14b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=3.8000002>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l1_loss(pred,target, reduction=\"sum\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a1a0f6b1-77ba-4908-9ed8-5bf2ab1f0032",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2,), dtype=float32, numpy=array([0.       , 3.8000002], dtype=float32)>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l1_loss(pred,target, reduction=\"none\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e3b223d-1a83-47b6-9087-cd6a09de5fe9",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### L2 Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2d9ee7e3-33b5-4f1c-a38f-f07f7aad973a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def l2_loss(predicted_values, target, reduction= \"sum\"):\n",
    "\n",
    "    # Calculate the difference between the pred and the actual values\n",
    "    difference = predicted_values - target\n",
    "    squared_difference = tf.square(difference)\n",
    "\n",
    "    # Sum over the four coordinates\n",
    "    errors = tf.reduce_sum(squared_difference,axis=-1)\n",
    "\n",
    "    # Reduction strategy\n",
    "    if reduction == \"sum\":\n",
    "        loss = tf.reduce_sum(errors)\n",
    "    elif reduction == \"max\":\n",
    "        loss = tf.reduce_max(errors)\n",
    "    elif reduction == \"mean\":\n",
    "        loss = tf.reduce_mean(errors)\n",
    "    else:\n",
    "        loss = errors\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "892945c0-f29f-438c-a11e-868df877dd16",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = tf.constant([[1., 2., 3., 4.],\n",
    "                    [0., 1., 0., 1.]], dtype=tf.float32)\n",
    "tgt  = tf.constant([[0., 0., 0., 0.],\n",
    "                    [0., 0., 0., 0.]], dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9882d87a-e7dd-4705-9c7e-3d735237e369",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2,), dtype=float32, numpy=array([30.,  2.], dtype=float32)>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l2_loss(pred, tgt, reduction=\"none\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cda946b3-c0fb-440b-89c6-95518335e08f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=32.0>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l2_loss(pred, tgt, reduction=\"sum\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e70bf3a3-8afc-4bb0-ae22-6bdd444607a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=16.0>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l2_loss(pred, tgt, reduction=\"mean\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "203e7ca6-64fe-4466-bff9-6c36f0500c8b",
   "metadata": {},
   "source": [
    "### Classification Losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b128a943-0ebf-4cea-b0fc-333cb1ab511f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_cross_entropy_loss(logits: tf.Tensor, labels: tf.Tensor, reduction: str =\"none\"):\n",
    "\n",
    "    logits = tf.cast(logits, tf.float32)\n",
    "    labels = tf.cast(labels, tf.int32)\n",
    "\n",
    "    # Calculating the shifted values\n",
    "    max_per_row = tf.reduce_max(logits, axis=-1,keepdims = True)\n",
    "\n",
    "    # Shifted logits from stopping exponential values from going extremely large\n",
    "    shifted_logits = logits - max_per_row\n",
    "    shifted_logits = tf.cast(shifted_logits,tf.float32)\n",
    "    \n",
    "    # Exponential Values\n",
    "    exponential_shifted_logits = tf.math.exp(shifted_logits)\n",
    "    sum_exponential_shifted_logits = tf.reduce_sum(exponential_shifted_logits, axis= -1, keepdims = True)\n",
    "\n",
    "    # Calculating probabilities\n",
    "    log_probabilities = tf.math.log(sum_exponential_shifted_logits)\n",
    "    log_probabilities = shifted_logits - log_probabilities\n",
    "\n",
    "    # Calculating for the labels\n",
    "    rows = tf.range(tf.shape(logits)[0])\n",
    "    index = tf.stack([rows,labels], axis=1)\n",
    "    probs = tf.gather_nd(log_probabilities, index)\n",
    "    per_class_loss = -probs\n",
    "\n",
    "    if reduction == \"sum\":\n",
    "        return tf.reduce_sum(per_class_loss)\n",
    "    elif reduction == \"mean\":\n",
    "        num = tf.cast(tf.size(per_class_loss),tf.float32)\n",
    "        sum_probs = tf.reduce_sum(per_class_loss)\n",
    "        return tf.math.divide_no_nan(sum_probs, num)\n",
    "    else:\n",
    "        return per_class_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d7549c64-ddc8-4336-8a14-fae383a14dd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = tf.constant(\n",
    "[\n",
    "  [ 2.0,  0.5, -1.0 ],\n",
    "  [ 0.0,  0.0,  0.0 ],\n",
    "  [ 1.0,  2.0,  3.0 ]\n",
    "]\n",
    ")\n",
    "labels = tf.constant([0, 1, 2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b0090788-f483-4ce9-8af7-d6a43ba45588",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=1.7475296>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "softmax_cross_entropy_loss(logits,labels,reduction=\"sum\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ef5a115a-4efc-45fa-9f35-26d8f72b1e95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=0.5825099>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "softmax_cross_entropy_loss(logits,labels,reduction=\"mean\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ee3fdd9e-4dfc-4d2a-97da-b7aa5959886f",
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = tf.constant([\n",
    "  [ 2.0,  0.5, -1.2 ],\n",
    "  [-1.0,  3.0,  0.2 ], \n",
    "  [ 0.0, -0.2,  1.0 ],\n",
    "  [ 4.5,  0.0, -2.0 ], \n",
    "  [ 1.2,  2.2, -3.0 ]]\n",
    ")\n",
    "labels = tf.constant([0, 1, 2, 0, 1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ad232be9-73b2-4721-b341-658588e64463",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=1.1524363>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "softmax_cross_entropy_loss(logits,labels,reduction=\"sum\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5ab0adad-07a8-409f-b78c-8697888cea20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=0.23048726>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "softmax_cross_entropy_loss(logits,labels,reduction=\"mean\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4bc186eb-4249-4039-8bc3-536eb69dda85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(5,), dtype=float32, numpy=\n",
       "array([0.23419617, 0.07615124, 0.5122688 , 0.01253359, 0.31728652],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "softmax_cross_entropy_loss(logits,labels,reduction=\"none\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd42761a-2d7f-4164-bf64-0ea2ba54003b",
   "metadata": {},
   "source": [
    "## Implementing the Multibox Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b4f48f02-be53-474d-ad31-6b432f5c6ad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_loc = tf.constant(\n",
    "[\n",
    "  [  # image 0\n",
    "    [ 0.10, -0.10,  0.20, -0.20 ],   # anchor 0\n",
    "    [ 0.00,  0.00,  0.00,  0.00 ],   # anchor 1\n",
    "    [ 0.30, -0.30,  0.10, -0.10 ],   # anchor 2\n",
    "  ]\n",
    "]\n",
    ")\n",
    "\n",
    "pred_logits = tf.constant(\n",
    "    [\n",
    "  [\n",
    "    [ 2.0,  0.5, -1.0 ],   # a0\n",
    "    [ 3.0, -1.0,  0.0 ],   # a1\n",
    "    [ 0.0, -0.5,  2.0 ],   # a2\n",
    "  ]\n",
    "]\n",
    ")\n",
    "\n",
    "tgt_loc = tf.constant(\n",
    "     [\n",
    "  [\n",
    "    [ 0.00,  0.00,  0.00,  0.00 ],   # a0 matched to some GT\n",
    "    [ 0.10, -0.10,  0.10, -0.10 ],   # a1 is background, these values ignored\n",
    "    [ 0.25, -0.25,  0.00,  0.00 ],   # a2 matched to some GT\n",
    "  ]\n",
    "]\n",
    ")\n",
    "\n",
    "tgt_labels = tf.constant(\n",
    "    [\n",
    "  [ 1, 0, 2 ]   # a0 is class 1, a1 is bg, a2 is class 2\n",
    "\n",
    "]\n",
    ")\n",
    "\n",
    "pos_mask = tf.constant(\n",
    "    [\n",
    "  [ True, False, True ]   # a0 & a2 are positives\n",
    "]\n",
    ")\n",
    "\n",
    "neg_mask = tf.constant(\n",
    "     [\n",
    "  [ False, True, False ]  # a1 is a mined negative\n",
    "]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6cd1b458-663f-48ce-8a66-f1bee544670c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def multibox_loss(predicted_offsets: tf.Tensor, predicted_logits: tf.Tensor, target_offsets: tf.Tensor, target_labels: tf.Tensor, positive_mask: tf.Tensor, negative_mask: tf.Tensor, localization_weight: float, classification_weight: float,beta: float|None ,cls_loss_type: str =\"softmax_ce\", loc_loss_type: str = \"smooth_l1\", normalize_denom: str = \"num_pos\", reduction: str = \"sum\"):\n",
    "    # Calculate the mask for classification of anchors\n",
    "    classification_mask = tf.logical_or(positive_mask,negative_mask)\n",
    "\n",
    "    # Calculate the number of positives and number of negative boxes\n",
    "    number_of_positives = tf.reduce_sum(tf.cast(positive_mask,tf.int32))\n",
    "    number_of_negatives = tf.reduce_sum(tf.cast(negative_mask,tf.int32))\n",
    "\n",
    "    # Calculating Safe values\n",
    "    number_of_positives = tf.maximum(1,number_of_positives)\n",
    "    number_of_negatives = tf.maximum(1,number_of_negatives)\n",
    "    number_of_classifications = number_of_positives + number_of_negatives\n",
    "\n",
    "    # Flattening the masks\n",
    "    B = tf.shape(positive_mask)[0]\n",
    "    N = tf.shape(positive_mask)[-1]\n",
    "\n",
    "    positive_mask_flattened = tf.reshape(positive_mask,[-1])\n",
    "    negative_mask_flattened = tf.reshape(negative_mask,[-1])\n",
    "    classification_mask_flattened = tf.reshape(classification_mask, [-1])\n",
    "\n",
    "    # Flattened Offsets\n",
    "    predicted_offsets_flattened = tf.reshape(predicted_offsets,[-1,4])\n",
    "    target_offsets_flattened = tf.reshape(target_offsets,[-1,4])\n",
    "\n",
    "    # Flattening the Logits\n",
    "    C = tf.shape(predicted_logits)[-1]\n",
    "    predicted_logits_flattened = tf.reshape(predicted_logits,[-1,C])\n",
    "\n",
    "    # Flattening the Labels\n",
    "    labels_flattened = tf.reshape(target_labels,[-1])\n",
    "\n",
    "    # Masking the offsets\n",
    "    positive_offsets_flattened = tf.boolean_mask(predicted_offsets_flattened,positive_mask_flattened)\n",
    "    negative_offsets_flattened = tf.boolean_mask(predicted_offsets_flattened,negative_mask_flattened)\n",
    "    positive_targets_flattened = tf.boolean_mask(target_offsets_flattened,positive_mask_flattened)\n",
    "\n",
    "    # Masking the logits\n",
    "    positive_logits_flattened = tf.boolean_mask(predicted_logits_flattened, positive_mask_flattened)\n",
    "    negative_logits_flattened = tf.boolean_mask(predicted_logits_flattened, negative_mask_flattened)\n",
    "    \n",
    "    # Selecting the anchors that are used in the classification task\n",
    "    selected_prediction_logits = tf.boolean_mask(predicted_logits_flattened,classification_mask_flattened)\n",
    "    selected_prediction_targets = tf.boolean_mask(labels_flattened,classification_mask_flattened)\n",
    "\n",
    "    # Calculating the losses for the model (Localization + Classification)\n",
    "\n",
    "    # The classification loss looks ath both the positive and negative anchors in the model\n",
    "    if cls_loss_type == \"softmax_ce\":\n",
    "        classification_raw = softmax_cross_entropy_loss(selected_prediction_logits,selected_prediction_targets,reduction=reduction)\n",
    "\n",
    "    \n",
    "    # The localization loss only looks at the positive \n",
    "    if loc_loss_type == \"smooth_l1\":\n",
    "        if beta != None:\n",
    "            localization_raw = smooth_l1_loss(positive_offsets_flattened,positive_targets_flattened,beta = beta, reduction=reduction)\n",
    "        else:\n",
    "            localization_raw = smooth_l1_loss(positive_offsets_flattened,positive_targets_flattened,beta = 1.0, reduction=reduction)\n",
    "        \n",
    "    elif loc_loss_type == \"l1_loss\":\n",
    "        localization_raw = l1_loss(positive_offsets_flattened,positive_targets_flattened,reduction=reduction)\n",
    "    elif loc_loss_type == \"l2_loss\":\n",
    "        localization_raw = l2_loss(positive_offsets_flattened,positive_targets_flattened,reduction=reduction)\n",
    "\n",
    "    # Normalize the losses\n",
    "    # Localization loss looks at the number of positives\n",
    "    if normalize_denom == \"num_neg\":\n",
    "        localization_loss = tf.math.divide(localization_raw,tf.cast(number_of_negatives,dtype=tf.float32))\n",
    "        classification_loss = tf.math.divide(classification_raw,tf.cast(number_of_negatives,dtype=tf.float32))\n",
    "    elif normalize_denom == \"num_cls\":\n",
    "        localization_loss = tf.math.divide(localization_raw,tf.cast(number_of_classifications,dtype=tf.float32))\n",
    "        classification_loss = tf.math.divide(classification_raw,tf.cast(number_of_classifications,dtype=tf.float32))\n",
    "    elif normalize_denom == \"num_batch\":\n",
    "        localization_loss = tf.math.divide(localization_raw,tf.cast(number_of_positives,dtype=tf.float32))\n",
    "        classification_loss = tf.math.divide(classification_raw,tf.cast(B,dtype=tf.float32))\n",
    "    else:\n",
    "        localization_loss = tf.math.divide(localization_raw,tf.cast(number_of_positives,dtype=tf.float32))\n",
    "        classification_loss = tf.math.divide(classification_raw,tf.cast(number_of_positives,dtype=tf.float32))\n",
    "\n",
    "    # Adding the losses using the weights\n",
    "    multibox_loss = (localization_weight * localization_loss) + (classification_weight * classification_loss)\n",
    "    \n",
    "    return {\n",
    "        'total_loss': multibox_loss,\n",
    "        'loc_loss': localization_loss,\n",
    "        'cls_loss': classification_loss,\n",
    "        'num_pos': number_of_positives,\n",
    "        'num_negative': number_of_negatives\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "06bdc57c-508f-408a-b54a-0931eed0a54d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'total_loss': <tf.Tensor: shape=(), dtype=float32, numpy=1.0332147>,\n",
       " 'loc_loss': <tf.Tensor: shape=(), dtype=float32, numpy=0.03125>,\n",
       " 'cls_loss': <tf.Tensor: shape=(), dtype=float32, numpy=1.0019647>,\n",
       " 'num_pos': <tf.Tensor: shape=(), dtype=int32, numpy=2>,\n",
       " 'num_negative': <tf.Tensor: shape=(), dtype=int32, numpy=1>}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multibox_loss(pred_loc, pred_logits,tgt_loc,tgt_labels, pos_mask,neg_mask,beta=1, localization_weight = 1, classification_weight = 1, loc_loss_type=\"smooth_l1\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e47838ae-0a0b-4fc8-9004-e02e17e7fb61",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
