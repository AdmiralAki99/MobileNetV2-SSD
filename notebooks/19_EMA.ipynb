{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "f91aca6c-d56a-466d-8f62-af358dd8be00",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_GPU_ALLOCATOR'] = 'cuda_malloc_async'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "03888e29-b03c-449e-9bfe-c5a6b332152d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from typing import Any, Optional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "9aeb14dc-c423-43a1-988d-e126597d49a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mobilenetv2ssd.core.config import load_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "618aa15e-f23e-4beb-9da9-7f5ff57dcb86",
   "metadata": {},
   "outputs": [],
   "source": [
    "main_cfg_path = \"configs/train/default.yaml\"\n",
    "model_cfg_path = \"configs/model/mobilenetv2_ssd_voc.yaml\"\n",
    "data_cfg_path = \"configs/data/voc_224.yaml\"\n",
    "eval_cfg_path = \"configs/eval/default.yaml\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "1415e1f1-3d20-4899-838b-7a6a29c8c828",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = load_config(main_cfg_path,model_cfg_path,data_cfg_path,eval_cfg_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "1e89ee26-6a80-4966-a94e-307d493484cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'enabled': True,\n",
       " 'decay': 0.9999,\n",
       " 'warmup_steps': 500,\n",
       " 'update_every': 1,\n",
       " 'eval_use_ema': True}"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config['train']['ema']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "9fdda7db-f793-4043-8d96-a6e24ff7dc1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EMAConfig(tf.Module):\n",
    "    def __init__(self, model: tf.keras.Model, ema_config: dict[str, Any]):\n",
    "        super().__init__(name=\"EMA\")\n",
    "\n",
    "        if model is None:\n",
    "            raise ValueError(\"EMA requires a built model instance (model cannot be None).\")\n",
    "        if not model.built:\n",
    "            raise ValueError(\"Build/call the model before creating EMA, otherwise trainable_variables is empty.\")\n",
    "        \n",
    "        self._decay = float(ema_config.get('decay', 0.999))\n",
    "        self._enabled = bool(ema_config.get('enabled', True))\n",
    "        \n",
    "        self._warmup_steps = int(ema_config.get('warmup_steps', 0))\n",
    "        self._update_every = int(ema_config.get('update_every', 1))\n",
    "        self._eval_use_ema = bool(ema_config.get('eval_use_ema', True))\n",
    "        self._is_applied = False\n",
    "        \n",
    "        self._num_updates = tf.Variable(0, dtype=tf.int64, trainable=False, name=\"num_updates\") # Tracking the counter\n",
    "        self._use_num_updates = bool(ema_config.get('use_num_updates', False))\n",
    "\n",
    "        # Need to initialize the model training variables\n",
    "        self._model_vars = list(model.trainable_variables)\n",
    "\n",
    "        self._ema_vars = [tf.Variable(tf.convert_to_tensor(variable), dtype = variable.dtype, trainable = False, name = f\"{variable.name.replace(':', '_')}_ema\") for variable in self._model_vars]\n",
    "\n",
    "        self._backup = None\n",
    "\n",
    "    def reset(self):\n",
    "        # The function needs to reset to the models current weights\n",
    "\n",
    "        # First check if the model weights and EMA weights are mapped 1:1\n",
    "        if len(self._ema_vars) != len(self._model_vars):\n",
    "            raise ValueError(\"EMA values are not 1:1 check the length of the variables passed to the EMA.\")\n",
    "\n",
    "        # Need to copy the current weights of the model into the EMA\n",
    "        for ema_var, model_var in zip(self._ema_vars,self._model_vars):\n",
    "            ema_var.assign(model_var)\n",
    "\n",
    "        # Need to reset the updates since the EMA was reset to the model's weights\n",
    "        self._num_updates.assign(0)\n",
    "\n",
    "        # Clearing the cache of values\n",
    "        self._backup = None\n",
    "\n",
    "    def should_update(self, step: int):\n",
    "\n",
    "        # Checking if the step is in the warmup phase or not\n",
    "        if step < self._warmup_steps:\n",
    "            return False\n",
    "\n",
    "        # Checking if the step is between the range acceptable\n",
    "        if self._update_every > 1 and step % self._update_every != 0:\n",
    "            return False\n",
    "\n",
    "        # Checking if EMA is enabled\n",
    "        if not self._enabled:\n",
    "            return False\n",
    "\n",
    "        return True # Everything passed the conditions\n",
    "\n",
    "    def update(self, step: int):\n",
    "        # Function updates the value of the EMA\n",
    "\n",
    "        decay = tf.constant(self._decay, tf.float32)\n",
    "        num_updates = tf.cast(self._num_updates, tf.float32)\n",
    "\n",
    "        # Calculating the ramp based on how may updates have been made to account for early garbage weights\n",
    "        adjusted_decay = (1 + num_updates)/ (10 + num_updates)\n",
    "\n",
    "        # Selecting the minimum of the two\n",
    "        decay_rate = tf.minimum(decay, adjusted_decay)\n",
    "        decay_rate = tf.cast(decay_rate, tf.float32)\n",
    "        \n",
    "        inverse_decay_rate = 1 - decay_rate\n",
    "        \n",
    "        # Now updating the value\n",
    "        for ema_var, model_var in zip(self._ema_vars,self._model_vars, strict = True):\n",
    "            decay_rate = tf.cast(decay_rate, ema_var.dtype)\n",
    "            inverse_decay_rate = tf.cast(1.0, ema_var.dtype) - decay_rate\n",
    "            ema_var.assign(decay_rate * ema_var + inverse_decay_rate * model_var)\n",
    "\n",
    "        # Increment the counter\n",
    "        self._num_updates.assign_add(1)\n",
    "\n",
    "    def apply_to(self, model: tf.keras.Model | None = None):\n",
    "        # Need to check if the model is None to pick the correct one\n",
    "        if model is None:\n",
    "            # Using the fallback model\n",
    "            model_variables = self._model_vars\n",
    "        else:\n",
    "            model_variables = list(model.trainable_variables)\n",
    "            \n",
    "        if len(self._ema_vars) != len(model_variables):\n",
    "            raise ValueError(\"EMA vars and model vars mismatch\")\n",
    "\n",
    "        # Checking if the dtype is correct\n",
    "        for ema_var, model_var in zip(self._ema_vars, model_variables):\n",
    "            if ema_var.dtype != model_var.dtype or ema_var.shape != model_var.shape:\n",
    "                raise ValueError(\"Dtypes not same for target model and EMA saved copy\")\n",
    "\n",
    "        # Now checking if backup exists so if used consecutively there can be a sort of ECF with the EMA weights\n",
    "        if (self._backup is not None) or (self._is_applied):\n",
    "            raise ValueError(\"Cannot apply since backup exists, restore() needs to be called\")\n",
    "\n",
    "        # Creating a backup\n",
    "        self._backup = [tf.convert_to_tensor(var) for var in model_variables]\n",
    "\n",
    "        # Now swapping the ema weights into the model weights\n",
    "        for ema_var, model_var in zip(self._ema_vars,model_variables):\n",
    "            model_var.assign(ema_var)\n",
    "\n",
    "        self._is_applied = True\n",
    "\n",
    "    def restore(self, model: tf.keras.Model | None = None):\n",
    "        # Need to check if the model is None to pick the correct one\n",
    "        if model is None:\n",
    "            # Using the fallback model\n",
    "            model_variables = self._model_vars\n",
    "        else:\n",
    "            model_variables = list(model.trainable_variables)\n",
    "\n",
    "        # Checking if there is a backup to restore from\n",
    "        if (self._backup is None) or (not self._is_applied):\n",
    "            raise ValueError(\"Cannot restore since backup doesnt exist, apply_to() needs to be called\")\n",
    "\n",
    "        if len(self._backup) != len(model_variables):\n",
    "            raise ValueError(\"Backup and model vars mismatch\")\n",
    "\n",
    "        # Restoring the backup to the model\n",
    "        for backup_var, model_var in zip(self._backup,model_variables):\n",
    "            if backup_var.dtype != model_var.dtype or backup_var.shape != model_var.shape:\n",
    "                raise ValueError(\"Dtypes not same for target model and Backup copy\")\n",
    "            model_var.assign(backup_var)\n",
    "\n",
    "        # Clearing the backup\n",
    "        self._backup = None\n",
    "        self._is_applied = False\n",
    "\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "447bf222-1503-4eae-9f8e-192fba2c789c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([tf.keras.layers.Input((4,)), tf.keras.layers.Dense(1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "f5af78fc-1e7b-4c33-b36b-92fb6348ba20",
   "metadata": {},
   "outputs": [],
   "source": [
    "ema = EMAConfig(model = model, ema_config = config['train']['ema'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "786ed938-f75f-4ea8-9327-afcf3c3329cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ema.should_update(500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "17640591-3b67-48a1-a13a-1e110efeb964",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(4, 1), dtype=float32, numpy=\n",
       "array([[ 0.93928623],\n",
       "       [-0.10844123],\n",
       "       [-0.42849827],\n",
       "       [ 0.73139715]], dtype=float32)>"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.convert_to_tensor(model.trainable_variables[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f0a6b68-5322-4fe2-b792-0060c0826128",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
