{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "915c11da-f47a-40fb-9665-bae6bf979a80",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_GPU_ALLOCATOR'] = 'cuda_malloc_async'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7aeb4e95-7f7f-4bf2-b920-1ff62709bd56",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-14 16:18:50.731552: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-12-14 16:18:50.753361: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-12-14 16:18:50.760004: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-12-14 16:18:50.776060: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-12-14 16:18:51.701013: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "from typing import List, Dict, Any\n",
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21983b17-da7a-4754-a8db-75e86d7b4210",
   "metadata": {},
   "source": [
    "## Types of Learning Schedules"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11742f98-5e26-42d5-b4ae-7669d2cc7794",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Cosine Warmup Learning Rate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "454945b6-c8e4-440c-a9b1-9db2873f7b77",
   "metadata": {},
   "source": [
    "$\n",
    "\\eta_t = \n",
    "\\begin{cases} \n",
    "\\eta_{max} \\cdot \\left(\\frac{t}{t_{warmup}}\\right) & \\text{if } t < t_{warmup} \\\\\n",
    "\\eta_{min} + \\frac{1}{2}(\\eta_{max} - \\eta_{min})\\left(1 + \\cos\\left(\\pi \\cdot \\frac{t - t_{warmup}}{t_{max} - t_{warmup}}\\right)\\right) & \\text{otherwise} \n",
    "\\end{cases}\n",
    "$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b605da60-0084-41c7-b398-1cd9719fb6f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CosineWarmupSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    def __init__(self, base_learning_rate: float, minimum_learning_rate: float, warmup_steps: int, total_steps:int):\n",
    "        super(CosineWarmupSchedule, self).__init__()\n",
    "        \n",
    "        self.base_learning_rate = tf.constant(base_learning_rate, dtype= tf.float32)\n",
    "        self.minimum_learning_rate = tf.constant(minimum_learning_rate, dtype= tf.float32)\n",
    "        self.warmup_steps = tf.constant(warmup_steps, dtype= tf.int64)\n",
    "        self.total_steps = tf.constant(total_steps, dtype= tf.int64)\n",
    "        self.pi = tf.constant(3.141592653589793, tf.float32)\n",
    "        \n",
    "    def __call__(self, step: tf.Tensor):\n",
    "        \n",
    "        step = tf.cast(step, dtype = tf.float32)\n",
    "\n",
    "        warmup_steps = tf.cast(self.warmup_steps, dtype = tf.float32)\n",
    "        total_steps = tf.cast(self.total_steps, dtype = tf.float32)\n",
    "\n",
    "        # Phase 1: Warm up learning rate\n",
    "        warmup_learning_rate = self.base_learning_rate * tf.minimum(1.0, step / tf.maximum(1.0, warmup_steps))\n",
    "\n",
    "        # Phase 2: Cosine Decay\n",
    "        diff_in_steps = tf.maximum(1.0,total_steps - warmup_steps)\n",
    "        progress = tf.clip_by_value((step - warmup_steps)/diff_in_steps, 0.0, 1.0)\n",
    "        cosine_decay = self.minimum_learning_rate + 0.5 * (self.base_learning_rate - self.minimum_learning_rate) * (1 + tf.math.cos(self.pi * progress))\n",
    "\n",
    "        return tf.where(step < warmup_steps, warmup_learning_rate, cosine_decay)       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b04c87f5-19f9-493b-9936-4f76102a7960",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1765747132.759508   18164 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "I0000 00:00:1765747132.845978   18164 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "I0000 00:00:1765747132.846041   18164 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "I0000 00:00:1765747132.847737   18164 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "I0000 00:00:1765747132.847797   18164 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "I0000 00:00:1765747132.847830   18164 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "I0000 00:00:1765747133.099800   18164 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "I0000 00:00:1765747133.099876   18164 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2025-12-14 16:18:53.099888: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2112] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "2025-12-14 16:18:53.099912: I tensorflow/core/common_runtime/gpu/gpu_process_state.cc:198] Using CUDA malloc Async allocator for GPU: 0\n",
      "I0000 00:00:1765747133.102311   18164 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2025-12-14 16:18:53.102385: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2021] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 9558 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4070, pci bus id: 0000:01:00.0, compute capability: 8.9\n"
     ]
    }
   ],
   "source": [
    "cosine_decay = CosineWarmupSchedule(base_learning_rate = 0.01, minimum_learning_rate = 0.0001, warmup_steps = 1000, total_steps = 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0b9b4bfc-86c7-4537-91dc-d4f951d9eab8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=1e-04>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_decay(10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbb16c9c-8f5a-4f4f-bfc8-78b981bb07fc",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Constant Learning Rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c99f01c0-e551-4818-be0c-bf335165145e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConstantLearningSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    def __init__(self, base_learning_rate: float):\n",
    "        super(ConstantLearningSchedule, self).__init__()\n",
    "        \n",
    "        self.base_learning_rate = tf.constant(base_learning_rate, dtype= tf.float32)\n",
    "        \n",
    "    def __call__(self, step: tf.Tensor):\n",
    "        return self.base_learning_rate      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c260e547-bd77-4e7a-8fe3-d0afddb12c3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "constant_lr = ConstantLearningSchedule(base_learning_rate = 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "72d15a74-f18b-457f-bbad-c36c618bacf1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=0.01>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "constant_lr(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdb46be8-7e26-49d9-b202-1e7c6df325eb",
   "metadata": {},
   "source": [
    "### Step Decay Learning Rate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bab236d8-e8d2-4be3-aee3-cf6f412a5979",
   "metadata": {},
   "source": [
    "$\n",
    "\\text{lr}=\\text{lr}_{\\text{initial}}\\times \\gamma^{n(t)}\n",
    "$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a3f29b97-9696-410e-b9be-fdb73cd72c17",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StepDecaySchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    def __init__(self, base_learning_rate: float, gamma: float, milestones: list[int]):\n",
    "        super(StepDecaySchedule, self).__init__()\n",
    "        \n",
    "        self.base_learning_rate = tf.constant(base_learning_rate, dtype= tf.float32)\n",
    "        self.gamma = tf.constant(gamma, dtype= tf.float32)\n",
    "        self.milestones = tf.constant(milestones, dtype= tf.int64)\n",
    "        \n",
    "    def __call__(self, step: tf.Tensor):\n",
    "        \n",
    "        step = tf.cast(step, dtype = tf.int64)\n",
    "\n",
    "        # Calculating the number of milestones passed\n",
    "        num_milestones = tf.reduce_sum(tf.cast(step >= self.milestones, dtype= tf.float32))\n",
    "\n",
    "        # Calculating the learning rate\n",
    "        learning_rate = self.base_learning_rate * tf.math.pow(self.gamma, num_milestones)\n",
    "\n",
    "        return learning_rate             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "328db1f2-8e56-4c87-88b6-1a147903300d",
   "metadata": {},
   "outputs": [],
   "source": [
    "step_decay = StepDecaySchedule(base_learning_rate = 0.01, gamma = 0.1, milestones = [10000, 20000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b1d1d5c3-b890-4545-addf-68bbf1090bd5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=1e-04>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "step_decay(20000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6da3edd6-f38f-46da-a60c-7d159c7fe619",
   "metadata": {},
   "source": [
    "### Exponential Decay Learning Rate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23bfe216-159f-4e9a-b88b-7cefd09fa6bf",
   "metadata": {},
   "source": [
    "$\n",
    "\\text{lr}=\\text{lr}_{\\text{initial}}\\times \\gamma^{\\frac{t}{k}}\n",
    "$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "69bb6f53-77c9-4ade-8ada-fad584d7c82b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExponentialDecaySchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    def __init__(self, base_learning_rate: float, gamma: float, decay_interval: int):\n",
    "        super(ExponentialDecaySchedule, self).__init__()\n",
    "        \n",
    "        self.base_learning_rate = tf.constant(base_learning_rate, dtype= tf.float32)\n",
    "        self.gamma = tf.constant(gamma, dtype= tf.float32)\n",
    "        self.decay_interval = tf.constant(decay_interval, dtype= tf.int64)\n",
    "        \n",
    "    def __call__(self, step: tf.Tensor):\n",
    "        \n",
    "        step = tf.cast(step, dtype = tf.int64)\n",
    "\n",
    "        # Calculating the decay factor\n",
    "        decay_factor = tf.math.pow(self.gamma, tf.cast(step/self.decay_interval, dtype = tf.float32))\n",
    "\n",
    "        # Calculating the exponential decay\n",
    "        learning_rate = self.base_learning_rate * decay_factor\n",
    "\n",
    "        return learning_rate          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "39e85b32-819b-4fcb-933d-c3e3823e7614",
   "metadata": {},
   "outputs": [],
   "source": [
    "exponential_decay = ExponentialDecaySchedule(base_learning_rate = 0.01, gamma = 0.9, decay_interval = 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e4a3c61d-bd43-4106-9636-90dbccbb4f98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=0.0034867835>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exponential_decay(10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d82b7f2f-44cb-47e1-bdf4-47e38a7ef9a8",
   "metadata": {},
   "source": [
    "## Scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8f45fd2c-cdcc-43f4-b072-f7c11ef51819",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Scheduler(tf.Module):\n",
    "    def __init__(self, optimizer: tf.keras.optimizers.Optimizer, lr_schedule: tf.keras.optimizers.schedules.LearningRateSchedule, start_step: int = 0):\n",
    "        super().__init__(name=\"scheduler\")\n",
    "        self.optimizer = optimizer\n",
    "        self.lr_schedule = lr_schedule\n",
    "        self.current_step = tf.Variable(start_step, dtype=tf.int64, trainable = False)\n",
    "\n",
    "    @tf.function\n",
    "    def apply_learning_rate(self):\n",
    "        learning_rate = self.lr_schedule(self.current_step)\n",
    "        learning_rate = tf.cast(learning_rate, dtype=tf.float32)\n",
    "        self.optimizer.learning_rate.assign(learning_rate)\n",
    "        \n",
    "        return learning_rate\n",
    "        \n",
    "    @tf.function   \n",
    "    def step(self):\n",
    "        \n",
    "        self.current_step.assign_add(1)\n",
    "        \n",
    "        learning_rate = self.lr_schedule(self.current_step)\n",
    "        \n",
    "        self.optimizer.learning_rate.assign(learning_rate)\n",
    "        \n",
    "        return learning_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6557f401-640d-4767-83ad-024414995504",
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = tf.keras.optimizers.Adam(learning_rate=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "317ececf-f887-43eb-af4e-6f4e36132c6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine_decay = CosineWarmupSchedule(base_learning_rate = 0.01, minimum_learning_rate = 0.0001, warmup_steps = 1000, total_steps = 10000)\n",
    "scheduler = Scheduler(optimizer = opt, lr_schedule = cosine_decay, start_step = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bf578e2d-1f7c-435e-8f18-357ffa12234e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=1.0000001e-05>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "dc201809-ae9a-4e3e-9ff0-beadd01a8347",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0000001e-05"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "opt.learning_rate.numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b6af275-2b5b-436e-84b2-176014426fa9",
   "metadata": {},
   "source": [
    "## Factory Pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "96da6691-3df9-4185-bf91-4af927a5284a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mobilenetv2ssd.core.config import load_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6d518ab7-5f0b-4d45-8478-4d3d8a83c4d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "main_cfg_path = \"configs/train/default.yaml\"\n",
    "model_cfg_path = \"configs/model/mobilenetv2_ssd_voc.yaml\"\n",
    "data_cfg_path = \"configs/data/voc_224.yaml\"\n",
    "eval_cfg_path = \"configs/eval/default.yaml\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "786a0144-e578-47ef-8871-2c9a8a830c6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = load_config(main_cfg_path,model_cfg_path,data_cfg_path,eval_cfg_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a9719aa4-4952-45fc-9043-8b0645be078f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'interval': 'step',\n",
       " 'name': 'cosine_warmup',\n",
       " 'base_lr': 0.001,\n",
       " 'min_lr': 1e-05,\n",
       " 'total_steps': 10000,\n",
       " 'warmup': {'enabled': True,\n",
       "  'epochs': 5,\n",
       "  'steps': 10,\n",
       "  'start_factor': 0.1,\n",
       "  'end_factor': 1.0,\n",
       "  'mode': 'linear'},\n",
       " 'step': {'drop_every_epochs': 10, 'gamma': 0.1},\n",
       " 'multistep': {'milestones_epochs': [30, 45], 'gamma': 0.1}}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config['train']['scheduler']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d721de7c-4765-470e-a6cd-05824c646e30",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_scheduler_config_from_main_config(config: dict[str,Any]):\n",
    "    scheduler = config['train'].get('scheduler')\n",
    "    warmup_config = scheduler.get('warmup', {})\n",
    "    step_config = scheduler.get('step', {})\n",
    "    multistep_config = scheduler.get('multistep', {})\n",
    "    \n",
    "    schedule_config = {\n",
    "        'learning_schedule': scheduler.get('name',\"constant\"),\n",
    "        'base_learning_rate': scheduler.get('base_lr', 0.1),\n",
    "        'mininum_learning_rate': scheduler.get('min_lr', 0.001),\n",
    "        'total_steps': scheduler.get('total_steps', None),\n",
    "        'warmup_epochs': warmup_config.get('epochs',10),\n",
    "        'warmup_steps': warmup_config.get('steps', 15),\n",
    "        'warmup_enabled': warmup_config.get('enabled',True),\n",
    "        'warmup_start_factor': warmup_config.get('start_factor',0.1),\n",
    "        'warmup_end_factor': warmup_config.get('end_factor',0.1),\n",
    "        'warmup_mode': warmup_config.get('mode',0.1),\n",
    "        'step_drop_every_epochs': step_config.get('drop_every_epochs', None),\n",
    "        'step_gamma': step_config.get('gamma', None),\n",
    "        'multistep_milestones': multistep_config.get('milestones_epochs', [])\n",
    "    }\n",
    "    return schedule_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c09a2a0b-2fe1-4893-a623-157f5149ca78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'learning_schedule': 'cosine_warmup',\n",
       " 'base_learning_rate': 0.001,\n",
       " 'mininum_learning_rate': 1e-05,\n",
       " 'total_steps': 10000,\n",
       " 'warmup_epochs': 5,\n",
       " 'warmup_steps': 10,\n",
       " 'warmup_enabled': True,\n",
       " 'warmup_start_factor': 0.1,\n",
       " 'warmup_end_factor': 1.0,\n",
       " 'warmup_mode': 'linear',\n",
       " 'step_drop_every_epochs': 10,\n",
       " 'step_gamma': 0.1,\n",
       " 'multistep_milestones': [30, 45]}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "create_scheduler_config_from_main_config(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "279ea187-cef8-4bf4-9074-2879bf03a913",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_learning_schedule(config: dict[str,Any]):\n",
    "    # Need to select the correct learning schedule\n",
    "    if config['learning_schedule'] == 'cosine_warmup':\n",
    "        learning_schedule = CosineWarmupSchedule(base_learning_rate = config['base_learning_rate'], minimum_learning_rate = config['mininum_learning_rate'], warmup_steps = config['warmup_steps'], total_steps = config['total_steps'])\n",
    "    elif config['learning_schedule'] == 'step_decay':\n",
    "        learning_schedule = StepDecaySchedule(base_learning_rate = config['base_learning_rate'], gamma = config['step_gamma'], milestones = config['multistep_milestones'])\n",
    "    elif config['learning_schedule'] == 'exponential_decay':\n",
    "        learning_schedule = ExponentialDecaySchedule(base_learning_rate = config['base_learning_rate'], gamma = config['step_gamma'], decay_interval = config['step_drop_every_epochs'])\n",
    "    elif config['learning_schedule'] == 'constant':\n",
    "        learning_schedule = ConstantLearningSchedule(base_learning_rate = config['base_learning_rate'])\n",
    "\n",
    "    return learning_schedule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0b23e606-0ac3-458b-a48c-1fd3061deb62",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_scheduler(config: dict[str,Any], optimizer: tf.keras.optimizers):\n",
    "    \n",
    "    scheduler_config = create_scheduler_config_from_main_config(config)\n",
    "\n",
    "    learning_schedule = build_learning_schedule(scheduler_config)\n",
    "\n",
    "    scheduler = Scheduler(optimizer = optimizer, lr_schedule = learning_schedule, start_step = 10)\n",
    "\n",
    "    return scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8e94a5b0-b3e7-4ea4-8d25-5665189774a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "scheduler = build_scheduler(config, optimizer = tf.keras.optimizers.Adam(learning_rate=0.01))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37a5937b-ae96-4339-9690-8e93d5a88049",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
