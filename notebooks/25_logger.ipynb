{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4456b70a-03e1-4324-ac81-50c26ec7031c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_GPU_ALLOCATOR'] = 'cuda_malloc_async'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "97d9abc3-564a-4756-a6b7-ab39a3892a51",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-30 00:26:38.570824: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2026-01-30 00:26:38.592016: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2026-01-30 00:26:38.597607: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2026-01-30 00:26:38.618200: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2026-01-30 00:26:40.247690: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from typing import Any"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5d2731a0-e301-4298-8410-0164128b57b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import logging\n",
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from typing import Any, Literal\n",
    "from dataclasses import dataclass, field\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6a1a189e-1b84-4f90-a5ad-11943bdc793b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Colours:\n",
    "    RESET = \"\\033[0m\"\n",
    "    BOLD = \"\\033[1m\"\n",
    "    DIM = \"\\033[2m\"\n",
    "    \n",
    "    # Standard colors\n",
    "    BLACK = \"\\033[30m\"\n",
    "    RED = \"\\033[31m\"\n",
    "    GREEN = \"\\033[32m\"\n",
    "    YELLOW = \"\\033[33m\"\n",
    "    BLUE = \"\\033[34m\"\n",
    "    MAGENTA = \"\\033[35m\"\n",
    "    CYAN = \"\\033[36m\"\n",
    "    WHITE = \"\\033[37m\"\n",
    "    \n",
    "    # Bright colors\n",
    "    BRIGHT_RED = \"\\033[91m\"\n",
    "    BRIGHT_GREEN = \"\\033[92m\"\n",
    "    BRIGHT_YELLOW = \"\\033[93m\"\n",
    "    BRIGHT_BLUE = \"\\033[94m\"\n",
    "    BRIGHT_MAGENTA = \"\\033[95m\"\n",
    "    BRIGHT_CYAN = \"\\033[96m\"\n",
    "    BRIGHT_WHITE = \"\\033[97m\"\n",
    "    \n",
    "    # Background colors\n",
    "    BG_RED = \"\\033[41m\"\n",
    "    BG_GREEN = \"\\033[42m\"\n",
    "    BG_YELLOW = \"\\033[43m\"\n",
    "    BG_BLUE = \"\\033[44m\"\n",
    "\n",
    "    @classmethod\n",
    "    def disable(cls):\n",
    "        for attr in dir(cls):\n",
    "            if not attr.startswith('_') and isinstance(getattr(cls, attr), str):\n",
    "                setattr(cls, attr, \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6c6c9e4d-a5a6-4406-9252-b5e45d85f538",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class LogLevel:\n",
    "    name: str\n",
    "    colour: str\n",
    "    icon: str\n",
    "    level: int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fceb5cef-ed4c-473d-b81d-22fc3232c5fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "LOG_LEVELS = {\n",
    "    \"debug\": LogLevel(\"DEBUG\", Colours.DIM, \"üîç\", logging.DEBUG),\n",
    "    \"info\": LogLevel(\"INFO\", Colours.BLUE, \"‚ÑπÔ∏è \", logging.INFO),\n",
    "    \"success\": LogLevel(\"SUCCESS\", Colours.BRIGHT_GREEN, \"‚úì\", logging.INFO + 1),\n",
    "    \"metric\": LogLevel(\"METRIC\", Colours.CYAN, \"üìä\", logging.INFO + 2),\n",
    "    \"warning\": LogLevel(\"WARNING\", Colours.BRIGHT_YELLOW, \"‚ö†Ô∏è \", logging.WARNING),\n",
    "    \"error\": LogLevel(\"ERROR\", Colours.BRIGHT_RED, \"‚úó\", logging.ERROR),\n",
    "    \"critical\": LogLevel(\"CRITICAL\", Colours.BG_RED + Colours.WHITE, \"üíÄ\", logging.CRITICAL),\n",
    "    \"checkpoint\": LogLevel(\"CHECKPOINT\", Colours.BRIGHT_GREEN, \"üíæ\", logging.INFO + 3),\n",
    "    \"epoch\": LogLevel(\"EPOCH\", Colours.BRIGHT_MAGENTA, \"üîÑ\", logging.INFO + 4),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2aa20bec-03b5-4243-b3df-12527efba892",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConsoleFormatter(logging.Formatter):\n",
    "    def __init__(self, frmt: str | None = None, date_frmt: str | None = None, use_colours: bool = True):\n",
    "        super().__init__(frmt,date_frmt)\n",
    "        self.use_colours = use_colours\n",
    "\n",
    "    def format(self, record: logging.LogRecord):\n",
    "        # Getting the logging info\n",
    "        level_name = record.levelname.lower()\n",
    "        level_config = LOG_LEVELS.get(level_name, LOG_LEVELS['info'])\n",
    "\n",
    "        # Checking if the colours are needed\n",
    "        if self.use_colours:\n",
    "\n",
    "            timestamp = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "            coloured_time = f\"{Colours.DIM}{timestamp}{Colours.RESET}\"\n",
    "\n",
    "            colourized_level = f\"{level_config.colour}{level_config.icon}{level_config.name:10}{Colours.RESET}\"\n",
    "\n",
    "            match level_name:\n",
    "                case \"error\" | \"critical\":\n",
    "                    coloured_msg = f\"{Colours.RED}{record.getMessage()}{Colours.RESET}\"\n",
    "                case \"success\":\n",
    "                    coloured_msg = f\"{Colours.GREEN}{record.getMessage()}{Colours.RESET}\"\n",
    "                case \"checkpoint\":\n",
    "                    coloured_msg = f\"{Colours.BRIGHT_GREEN}{record.getMessage()}{Colours.RESET}\"\n",
    "                case \"warning\":\n",
    "                    coloured_msg = f\"{Colours.YELLOW}{record.getMessage()}{Colours.RESET}\"\n",
    "                case \"metric\":\n",
    "                    coloured_msg = f\"{Colours.CYAN}{record.getMessage()}{Colours.RESET}\"\n",
    "                case \"epoch\":\n",
    "                    coloured_msg = f\"{Colours.MAGENTA}{record.getMessage()}{Colours.RESET}\"\n",
    "                case _:\n",
    "                    coloured_msg = record.getMessage()\n",
    "\n",
    "            return f\"{coloured_time} | {colourized_level} | {coloured_msg}\"\n",
    "        else:\n",
    "            super().format(record)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e35e5b93-a6e3-4c11-9c17-8fa9b937b8a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FileFormatter(logging.Formatter):\n",
    "    def format(self,record: logging.LogRecord):\n",
    "        timestamp = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S.%f\")[:-3]\n",
    "        level = record.levelname\n",
    "\n",
    "        # Including extra stuff I might want to add later\n",
    "        extra = \"\"\n",
    "        if hasattr(record,\"step\"):\n",
    "            extra = extra + f\" [step={record.step}\"\n",
    "        if hasattr(record,\"epoch\"):\n",
    "            extra = extra + f\" [epoch={record.epoch}\"\n",
    "\n",
    "        return f\"{timestamp} | {level:10} | {record.getMessage()}{extra}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6517a17d-1af1-4e83-bc79-5712f7207ca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TensorBoardWriter:\n",
    "    def __init__(self, log_directory: Path):\n",
    "        self.log_directory = log_directory\n",
    "        self._writer = tf.summary.create_file_writer(str(log_directory))\n",
    "\n",
    "    @property\n",
    "    def writer(self):\n",
    "        return self._writer\n",
    "\n",
    "    def scalar(self, tag: str, value: float, step: int):\n",
    "        with self.writer.as_default(step = step):\n",
    "            tf.summary.scalar(tag, value)\n",
    "\n",
    "    def scalars(self, main_tag: str, values: dict[str, float], step: int):\n",
    "        with self.writer.as_default(step = step):\n",
    "            for name, value in values.items():\n",
    "                # Writing the scalars to the tensorboard\n",
    "                tf.summary.scalar(f\"{main_tag}/{name}\", value)\n",
    "\n",
    "    def image(self, tag: str, image: tf.Tensor | np.ndarray, step: int):\n",
    "        with self._writer.as_default(step = step):\n",
    "            # Writing the image to the tensorboard\n",
    "            if len(image.shape) == 3:\n",
    "                image = tf.expand_dims(image,axis = 0)\n",
    "\n",
    "            tf.summary.image(tag, image)\n",
    "\n",
    "    def histogram(self, tag: str, values: tf.Tensor | np.ndarray, step : int):\n",
    "        with self._writer.as_default(step = step):\n",
    "            tf.summary.histogram(tag, values)\n",
    "\n",
    "    def text(self, tag: str, text: str, step: int):\n",
    "        with self._writer.as_default(step = step):\n",
    "            tf.summary.text(tag, text)\n",
    "\n",
    "    def flush(self):\n",
    "        self._writer.flush()\n",
    "\n",
    "    def close(self):\n",
    "        self._writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "82908791-3931-4a7b-a06b-db31be8df314",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Logger:\n",
    "    def __init__(self, job_name: str, log_dir: str | Path = \"logs\", tensorboard: bool = True, console: bool = True, file: bool = True, level: str = \"info\", config: dict | None = None):\n",
    "        \n",
    "        self.job_name = job_name\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "        self.job_dir = Path(log_dir) / f\"{job_name}_{timestamp}\"\n",
    "        self.job_dir.mkdir(parents = True, exist_ok = True)\n",
    "        \n",
    "        self.tensorboard_dir = self.job_dir / \"tensorboard\"\n",
    "        self.checkpoints_dir = self.job_dir / \"checkpoints\"\n",
    "        self.checkpoints_dir.mkdir(exist_ok = True)\n",
    "\n",
    "        self._logger = logging.getLogger(f\"training.{job_name}.{timestamp}\")\n",
    "        self._logger.setLevel(logging.DEBUG)\n",
    "        self._logger.handlers.clear()\n",
    "        self._logger.propogate = False\n",
    "\n",
    "        self._console_logging_enabled = console\n",
    "        if console:\n",
    "            console_logging_handler = logging.StreamHandler(sys.stdout)\n",
    "            console_logging_handler.setLevel(LOG_LEVELS.get(level, LOG_LEVELS['info']).level)\n",
    "            console_logging_handler.setFormatter(ConsoleFormatter())\n",
    "            self._logger.addHandler(console_logging_handler)\n",
    "\n",
    "        self._file_logging_enabled = file\n",
    "        if file:\n",
    "            log_file = self.job_dir / \"training.log\"\n",
    "            file_logging_handler = logging.FileHandler(log_file, encoding = \"utf-8\")\n",
    "            file_logging_handler.setLevel(logging.DEBUG)\n",
    "            file_logging_handler.setFormatter(FileFormatter())\n",
    "            self._logger.addHandler(file_logging_handler)\n",
    "\n",
    "        self._tensorboard_writer: TensorBoardWriter | None = None\n",
    "        if tensorboard:\n",
    "            self.tensorboard_dir.mkdir(exist_ok = True)\n",
    "            self._tensorboard_writer = TensorBoardWriter(self.tensorboard_dir)\n",
    "\n",
    "        # Storing a metric history\n",
    "        self._metric_history: list[dict] = []\n",
    "\n",
    "        # Saving the config file snapshot for examination too\n",
    "        if config:\n",
    "            config_path = self.job_dir / \"config.json\"\n",
    "            with open(config_path, \"w\") as file:\n",
    "                json.dump(config, file, indent = 2, default = str)\n",
    "\n",
    "        self.info(f\"Logger Initialized: {self.job_dir}\")\n",
    "\n",
    "    def _log(self, level: str, message: str, **extra):\n",
    "        level_config = LOG_LEVELS.get(level, LOG_LEVELS[\"info\"])\n",
    "\n",
    "        record = self._logger.makeRecord(name = self._logger.name, level = level_config.level, fn = \"\", lno = 0, msg = message, args = (), exc_info = None)\n",
    "\n",
    "        record.level_name = level.upper()\n",
    "        for key, value in extra.items():\n",
    "            setattr(record, key, value)\n",
    "\n",
    "        self._logger.handle(record)\n",
    "\n",
    "    def debug(self, message: str, **extra):\n",
    "        self._log(\"debug\", message, **extra)\n",
    "\n",
    "    def info(self, message: str, **extra):\n",
    "        self._log(\"info\", message, **extra)\n",
    "\n",
    "    def success(self, message: str, **extra):\n",
    "        self._log(\"success\", message, **extra)\n",
    "\n",
    "    def warning(self, message: str, **extra):\n",
    "        self._log(\"warning\", message, **extra)\n",
    "\n",
    "    def critical(self, message: str, **extra):\n",
    "        self._log(\"critical\", message, **extra)\n",
    "\n",
    "    def error(self, message: str, **extra):\n",
    "        self._log(\"error\", message, **extra)\n",
    "\n",
    "    def checkpoint(self, message: str, path: str | Path | None = None, **extra):\n",
    "        full_message = f\"{message} -> {path}\" if path else message\n",
    "        self._log(\"checkpoint\", message, **extra)\n",
    "\n",
    "    def epoch(self, epoch: int, total: int | None = None, **extra):\n",
    "        message = f\"Epoch {epoch}/{total}\" if total else f\"Epoch {epoch}\"\n",
    "        self._log(\"epoch\", message, epoch = epoch, **extra)\n",
    "\n",
    "    def metric(self, message: str, **extra):\n",
    "        self._log(\"metric\", message, **extra)\n",
    "\n",
    "    def log_scalar(self, tag: str, value: float, step: int):\n",
    "\n",
    "        if self._tensorboard_writer:\n",
    "            self._tensorboard_writer.scalar(tag, value, step)\n",
    "\n",
    "    def log_scalars(self, tag: str, values: dict[str, float], step: int):\n",
    "        if self._tensorboard_writer:\n",
    "            self._tensorboard_writer.scalars(tag, values, step)\n",
    "\n",
    "    def log_image(self, tag: str, image: tf.Tensor | np.ndarray, step: int):\n",
    "        if self._tensorboard_writer:\n",
    "            self._tensorboard_writer.image(tag, image, step)\n",
    "\n",
    "    def log_histogram(self, tag: str, values: tf.Tensor | np.ndarray, step: int):\n",
    "        if self._tensorboard_writer:\n",
    "            self._tensorboard_writer.histogram(tag, image, step)\n",
    "\n",
    "    def log_text(self, tag: str, text: str, step: int):\n",
    "        if self._tensorboard_writer:\n",
    "            self._tensorboard_writer.text(tag, image, step)\n",
    "\n",
    "    def log_metrics(self, metrics: dict[str, float], step: int, prefix: str = \"\", to_tensorboard: bool = True, to_console: bool = True):\n",
    "        if prefix:\n",
    "            prefixed = {f\"{prefix}/{key}\": value for key, value in metrics.items()}\n",
    "        else:\n",
    "            prefixed = metrics\n",
    "\n",
    "        if to_tensorboard and self._tensorboard_writer:\n",
    "            # Write to tensorboard\n",
    "            for tag, value in prefixed.items():\n",
    "                self._tensorboard_writer.scalar(tag, value, step)\n",
    "\n",
    "        if to_console:\n",
    "            metrics_message = \" | \".join(f\"{key}: {value:.4f}\" for key, value in prefixed.items())\n",
    "            self.metric(f\"[Step {step}] {metrics_message}\", step = step)\n",
    "\n",
    "        # Adding to the metric history\n",
    "        self._metric_history.append({\n",
    "            \"step\": step,\n",
    "            \"timestamp\": datetime.now().isoformat(),\n",
    "            **prefixed\n",
    "        })\n",
    "\n",
    "    def log_training_step(self, step: int, loss: float, learning_rate: float, extra: dict[str,float] | None = None, log_every: int = 100):\n",
    "\n",
    "        # Checking if the step needs to be logged\n",
    "        if step % log_every != 0:\n",
    "            return\n",
    "\n",
    "        metrics = {\"loss\": loss, \"lr\": learning_rate}\n",
    "        if extra:\n",
    "            metrics.update(extra)\n",
    "\n",
    "        self.log_metrics(metrics, step, prefix = \"train\", to_console = True)\n",
    "\n",
    "    def log_validation(self, metrics: dict[str, float], step: int):\n",
    "\n",
    "        self.log_metrics(metrics, step = step, prefix = \"val\", to_console = True)\n",
    "\n",
    "        # Checking to highlight classic metrics\n",
    "        for key in [\"mAP@0.50\", \"mAP\", \"AP\"]:\n",
    "            if key in metrics:\n",
    "                self.success(f\"Validation {key}: {metrics[key]:.4f}\")\n",
    "                break\n",
    "\n",
    "    def log_epoch_summary(self, epoch: int, train_metrics: dict[str,float], val_metrics: dict[str, float] | None = None):\n",
    "\n",
    "        # Line divider\n",
    "        self.info(f\"{'-' * 50}\")\n",
    "\n",
    "        training_message = \" | \".join(f\"{key}: {value:.4f}\" for key,value in train_metrics.items())\n",
    "        self.info(f\"Epoch {epoch} Train: {training_message}\")\n",
    "\n",
    "        if val_metrics:\n",
    "            validation_message = \" | \".join(f\"{key}: {value:.4f}\" for key,value in val_metrics.items())\n",
    "            self.info(f\"Epoch {epoch} Val: {validation_message}\")\n",
    "\n",
    "        self.info(f\"{'-' * 50}\")\n",
    "\n",
    "    def get_checkpoint_path(self, filename: str):\n",
    "        return self.checkpoints_dir / filename\n",
    "\n",
    "    def save_metric_history(self):\n",
    "        path = self.job_dir / \"metric_history.json\"\n",
    "        with open(path, \"w\") as file:\n",
    "            json.dump(self._metric_history, file, indent = 2)\n",
    "\n",
    "    def flush(self):\n",
    "        # Flushing each handler\n",
    "        for handler in self._logger.handlers:\n",
    "            handler.flush()\n",
    "\n",
    "        if self._tensorboard_writer:\n",
    "            self._tensorboard_writer.flush()\n",
    "\n",
    "    def close(self):\n",
    "\n",
    "        # Wrapping up everything\n",
    "        self.save_metric_history()\n",
    "\n",
    "        if self._tensorboard_writer:\n",
    "            self._tensorboard_writer.close()\n",
    "\n",
    "        for handler in self._logger.handlers:\n",
    "            handler.close()\n",
    "            self._logger.removeHandler(handler)\n",
    "\n",
    "\n",
    "    def __enter__(self):\n",
    "        return self\n",
    "\n",
    "    def __exit__(self, exc_type, exc_val, exc_tb):\n",
    "        if exc_type:\n",
    "            self.error(f\"Exception: {exc_type.__name__}: {exc_val}\")\n",
    "\n",
    "        self.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "782d7249-f25c-4a49-9efb-0a375318ef9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_logger_from_config(config: dict, job_name: str | None = None):\n",
    "    logging_config = config.get('logging', {})\n",
    "\n",
    "    return Logger(job_name = job_name, log_dir = logging_config.get('log_dir', \"logs\"), tensorboard = logging_config.get('tensorboard', True), console = logging_config.get('console', True), file = logging_config.get('file', True), level = logging_config.get('level', \"info\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "45da3f4e-d25b-4722-aefd-2a7a01808a97",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1769750801.228972   21028 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "I0000 00:00:1769750801.381298   21028 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "I0000 00:00:1769750801.381401   21028 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "I0000 00:00:1769750801.408659   21028 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "I0000 00:00:1769750801.408846   21028 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "I0000 00:00:1769750801.408917   21028 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "I0000 00:00:1769750801.619740   21028 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "I0000 00:00:1769750801.619896   21028 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2026-01-30"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m2026-01-30 00:26:41\u001b[0m | \u001b[34m‚ÑπÔ∏è INFO      \u001b[0m | Logger Initialized: logs/demo_20260130_002641\n",
      "\u001b[2m2026-01-30 00:26:41\u001b[0m | \u001b[34m‚ÑπÔ∏è INFO      \u001b[0m | Info message\n",
      "\u001b[2m2026-01-30 00:26:41\u001b[0m | \u001b[34m‚ÑπÔ∏è INFO      \u001b[0m | Success message\n",
      "\u001b[2m2026-01-30 00:26:41\u001b[0m | \u001b[93m‚ö†Ô∏è WARNING   \u001b[0m | \u001b[33mWarning message\u001b[0m\n",
      "\u001b[2m2026-01-30 00:26:41\u001b[0m | \u001b[91m‚úóERROR     \u001b[0m | \u001b[31mError message\u001b[0m\n",
      "\n",
      "\u001b[2m2026-01-30 00:26:41\u001b[0m | \u001b[34m‚ÑπÔ∏è INFO      \u001b[0m | Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 00:26:41.619915: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2112] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "2026-01-30 00:26:41.619963: I tensorflow/core/common_runtime/gpu/gpu_process_state.cc:198] Using CUDA malloc Async allocator for GPU: 0\n",
      "I0000 00:00:1769750801.621536   21028 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2026-01-30 00:26:41.621613: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2021] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 9558 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4070, pci bus id: 0000:01:00.0, compute capability: 8.9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m2026-01-30 00:26:42\u001b[0m | \u001b[34m‚ÑπÔ∏è INFO      \u001b[0m | [Step 0] train/loss: 1.0000 | train/lr: 0.0010\n",
      "\u001b[2m2026-01-30 00:26:42\u001b[0m | \u001b[34m‚ÑπÔ∏è INFO      \u001b[0m | [Step 50] train/loss: 0.0196 | train/lr: 0.0010\n",
      "\u001b[2m2026-01-30 00:26:42\u001b[0m | \u001b[34m‚ÑπÔ∏è INFO      \u001b[0m | [Step 1] val/mAP@0.50: 0.6000 | val/mAP@0.75: 0.3500\n",
      "\u001b[2m2026-01-30 00:26:42\u001b[0m | \u001b[34m‚ÑπÔ∏è INFO      \u001b[0m | Validation mAP@0.50: 0.6000\n",
      "\u001b[2m2026-01-30 00:26:42\u001b[0m | \u001b[34m‚ÑπÔ∏è INFO      \u001b[0m | Epoch 2/3\n",
      "\u001b[2m2026-01-30 00:26:42\u001b[0m | \u001b[34m‚ÑπÔ∏è INFO      \u001b[0m | [Step 100] train/loss: 0.0099 | train/lr: 0.0010\n",
      "\u001b[2m2026-01-30 00:26:42\u001b[0m | \u001b[34m‚ÑπÔ∏è INFO      \u001b[0m | [Step 150] train/loss: 0.0066 | train/lr: 0.0010\n",
      "\u001b[2m2026-01-30 00:26:42\u001b[0m | \u001b[34m‚ÑπÔ∏è INFO      \u001b[0m | [Step 2] val/mAP@0.50: 0.7000 | val/mAP@0.75: 0.4000\n",
      "\u001b[2m2026-01-30 00:26:42\u001b[0m | \u001b[34m‚ÑπÔ∏è INFO      \u001b[0m | Validation mAP@0.50: 0.7000\n",
      "\u001b[2m2026-01-30 00:26:42\u001b[0m | \u001b[34m‚ÑπÔ∏è INFO      \u001b[0m | Best model\n",
      "\u001b[2m2026-01-30 00:26:42\u001b[0m | \u001b[34m‚ÑπÔ∏è INFO      \u001b[0m | Epoch 3/3\n",
      "\u001b[2m2026-01-30 00:26:42\u001b[0m | \u001b[34m‚ÑπÔ∏è INFO      \u001b[0m | [Step 200] train/loss: 0.0050 | train/lr: 0.0010\n",
      "\u001b[2m2026-01-30 00:26:42\u001b[0m | \u001b[34m‚ÑπÔ∏è INFO      \u001b[0m | [Step 250] train/loss: 0.0040 | train/lr: 0.0010\n",
      "\u001b[2m2026-01-30 00:26:42\u001b[0m | \u001b[34m‚ÑπÔ∏è INFO      \u001b[0m | [Step 3] val/mAP@0.50: 0.8000 | val/mAP@0.75: 0.4500\n",
      "\u001b[2m2026-01-30 00:26:42\u001b[0m | \u001b[34m‚ÑπÔ∏è INFO      \u001b[0m | Validation mAP@0.50: 0.8000\n",
      "\u001b[2m2026-01-30 00:26:42\u001b[0m | \u001b[34m‚ÑπÔ∏è INFO      \u001b[0m | Training complete!\n",
      "\n",
      "‚úì Demo complete\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with Logger(\"demo\", log_dir= Path(\"logs\")) as logger:\n",
    "    logger.debug(\"Debug message (only in file)\")\n",
    "    logger.info(\"Info message\")\n",
    "    logger.success(\"Success message\")\n",
    "    logger.warning(\"Warning message\")\n",
    "    logger.error(\"Error message\")\n",
    "\n",
    "    print()\n",
    "\n",
    "    for epoch in range(1, 4):\n",
    "        logger.epoch(epoch, total=3)\n",
    "            \n",
    "        for step in range(100):\n",
    "            global_step = (epoch - 1) * 100 + step\n",
    "            loss = 1.0 / (global_step + 1)\n",
    "                \n",
    "            # Rate-limited logging\n",
    "            logger.log_training_step(\n",
    "                step=global_step,\n",
    "                loss=loss,\n",
    "                learning_rate=0.001,\n",
    "                log_every=50,\n",
    "            )\n",
    "            \n",
    "        # Validation\n",
    "        logger.log_validation(\n",
    "            {\"mAP@0.50\": 0.5 + epoch * 0.1, \"mAP@0.75\": 0.3 + epoch * 0.05},\n",
    "            step=epoch,\n",
    "        )\n",
    "            \n",
    "        # Checkpoint\n",
    "        if epoch == 2:\n",
    "            logger.checkpoint(\"Best model\", path=\"model_best.h5\")\n",
    "        \n",
    "    logger.success(\"Training complete!\")\n",
    "print(\"\\n‚úì Demo complete\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
