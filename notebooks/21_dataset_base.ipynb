{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "083dacdb-1f70-4163-9ec0-bd67913c0aa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_GPU_ALLOCATOR'] = 'cuda_malloc_async'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "02825d4e-245c-41b8-be76-65ebc8e8081e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-13 22:32:20.597519: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2026-01-13 22:32:20.618869: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2026-01-13 22:32:20.625197: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2026-01-13 22:32:20.641751: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2026-01-13 22:32:21.824164: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from typing import Any\n",
    "from pathlib import Path\n",
    "from abc import ABC, abstractmethod\n",
    "import hashlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ecbf1c5a-bc2c-4863-8501-47c4471e043e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mobilenetv2ssd.core.config import load_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "02eec859-1163-482d-b30e-22b1713cb7ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "main_cfg_path = \"configs/train/default.yaml\"\n",
    "model_cfg_path = \"configs/model/mobilenetv2_ssd_voc.yaml\"\n",
    "data_cfg_path = \"configs/data/voc_224.yaml\"\n",
    "eval_cfg_path = \"configs/eval/default.yaml\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d6e59868-3ff0-4556-919c-693928d1f7af",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = load_config(main_cfg_path,model_cfg_path,data_cfg_path,eval_cfg_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e5bf4df3-a07b-4e4f-846c-a174390ac54b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dataset_name': 'voc',\n",
       " 'root': '/mnt/d/dev/MobileNetV2-SSD/datasets/VOCdevkit/VOC2012',\n",
       " 'train_split': 'trainval',\n",
       " 'val_split': 'val',\n",
       " 'input_size': [224, 224],\n",
       " 'num_workers': 4,\n",
       " 'shuffle_buffer': 1000,\n",
       " 'prefetch_batches': 2,\n",
       " 'standardize': {'boxes_format_in': 'xyxy_pixels',\n",
       "  'boxes_format_out': 'xyxy_pixels',\n",
       "  'image_rgb': True,\n",
       "  'to_float32': True,\n",
       "  'scale': '0_1'},\n",
       " 'preprocess': {'standardize_pipeline': ['to_float32', 'scale_01'],\n",
       "  'pipeline': ['resize', 'sanitize_boxes', 'normalize'],\n",
       "  'params': {'resize': {'enabled': True,\n",
       "    'size': [300, 300],\n",
       "    'mode': 'stretch',\n",
       "    'interp': 'bilinear'},\n",
       "   'sanitize_boxes': {'enabled': True,\n",
       "    'clip': True,\n",
       "    'min_size': 1,\n",
       "    'min_size_mode': 'pixels'},\n",
       "   'normalize': {'enabled': True,\n",
       "    'mean': [0.485, 0.456, 0.406],\n",
       "    'std': [0.229, 0.224, 0.225]}}},\n",
       " 'augment': {'enabled': True,\n",
       "  'output_box_norm': True,\n",
       "  'pipeline': ['photometric_distort',\n",
       "   'random_expand',\n",
       "   'random_iou_crop',\n",
       "   'random_flip'],\n",
       "  'params': {'random_flip': {'enabled': True,\n",
       "    'prob': 0.5,\n",
       "    'direction': 'horizontal'},\n",
       "   'random_iou_crop': {'enabled': False,\n",
       "    'prob': 1.0,\n",
       "    'min_iou_choices': [0.1, 0.3, 0.5, 0.7, 0.9, None],\n",
       "    'min_scale': 0.3,\n",
       "    'max_scale': 1.0,\n",
       "    'max_attempts': 50,\n",
       "    'fallback': 'original'},\n",
       "   'random_expand': {'enabled': False,\n",
       "    'prob': 0.5,\n",
       "    'max_ratio': 4.0,\n",
       "    'fill': 'mean',\n",
       "    'value': [0.485, 0.456, 0.406]},\n",
       "   'photometric_distort': {'enabled': False,\n",
       "    'prob': 0.5,\n",
       "    'brightness': 0.125,\n",
       "    'contrast': [0.5, 1.5],\n",
       "    'saturation': [0.5, 1.5],\n",
       "    'hue': 0.05,\n",
       "    'random_order': True}}},\n",
       " 'val_preprocess': {'output_box_norm': True,\n",
       "  'standardize_pipeline': ['to_float32', 'scale_01'],\n",
       "  'pipeline': ['resize', 'sanitize_boxes', 'normalize'],\n",
       "  'params': {'resize': {'enabled': True,\n",
       "    'size': [224, 224],\n",
       "    'mode': 'stretch',\n",
       "    'interp': 'bilinear'},\n",
       "   'sanitize_boxes': {'enabled': True,\n",
       "    'clip': True,\n",
       "    'min_size': 1,\n",
       "    'min_size_mode': 'pixels'},\n",
       "   'normalize': {'enabled': True,\n",
       "    'mean': [0.485, 0.456, 0.406],\n",
       "    'std': [0.229, 0.224, 0.225]}}},\n",
       " 'classes_file': '/mnt/d/dev/MobileNetV2-SSD/datasets/VOCdevkit/labels/voc_labels.txt'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config['data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "25078399-ef04-4bbd-807a-12c74932ad82",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseDetectionDataset(ABC):\n",
    "    def __init__(self, config: dict[str,Any], split: str, transform = None):\n",
    "        self._dataset_config = self.extract_dataset_config(config)\n",
    "        self._labels = self.load_classes(self._dataset_config.get('classes_file', ''))\n",
    "        self._id_to_name = {index + 1: element for index, element in enumerate(self._labels)}\n",
    "        self._id_to_name[0] = \"background\"\n",
    "        self._name_to_id = {element: index + 1 for index, element in enumerate(self._labels)}\n",
    "        self._name_to_id[\"background\"] = 0\n",
    "        self._num_classes = len(self._labels) + 1\n",
    "\n",
    "        self._split = split\n",
    "        self._is_train = (split in (\"train\", \"trainval\"))\n",
    "        self._transform = transform\n",
    "\n",
    "    def extract_dataset_config(self, config: dict[str, Any]):\n",
    "        data_config = config['data']\n",
    "        augment_config = data_config.get('augment',{})\n",
    "        normalization_config = data_config.get('normalization',{})\n",
    "        \n",
    "        tranform_config = {\n",
    "            'random_flip': augment_config.get('random_flip', False),\n",
    "            'random_flip_prob': augment_config.get('random_flip_prob', 0.5),\n",
    "            'random_crop': augment_config.get('random_crop', False),\n",
    "            'min_crop_iou_choices': augment_config.get('min_crop_iou_choices', []),\n",
    "            'min_crop_scale': augment_config.get('min_crop_scale', 0.3),\n",
    "            'max_crop_scale': augment_config.get('max_crop_scale', 1.0),\n",
    "            'photometric_distort': augment_config.get('photometric_distort', False),\n",
    "            'photometric_distort_prob': augment_config.get('photometric_distort_prob', 0.5),\n",
    "            'normalization_mean': normalization_config.get('mean', [0.5,0.5,0.5]),\n",
    "            'normalization_std': normalization_config.get('std', [0.25, 0.25, 0.25]),\n",
    "        }\n",
    "\n",
    "        dataset_config = {\n",
    "            'root': data_config.get('root', ''),\n",
    "            'input_size': tuple(data_config.get('input_size', [300,300])),\n",
    "            'transform_opts': tranform_config,\n",
    "            'classes_file': data_config.get('classes_file', '')\n",
    "        }\n",
    "\n",
    "        return dataset_config\n",
    "\n",
    "    def load_classes(self, label_file_path: str | Path):\n",
    "        if isinstance(label_file_path, str):\n",
    "            label_file_path = Path(label_file_path)\n",
    "\n",
    "        # Now reading the file and then loading it in\n",
    "        with open(label_file_path, \"r\") as f:\n",
    "            labels = [line.strip() for line in f.readlines() if line.strip()]\n",
    "\n",
    "        return labels\n",
    "\n",
    "    @abstractmethod\n",
    "    def __len__(self) -> int:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    @abstractmethod\n",
    "    def _load_raw_sample(self, index: int):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def _standardize_target(self, image: tf.Tensor, target : dict, index: int):\n",
    "        # Checking for keys in the targets\n",
    "        validation_keys = ['boxes','labels']\n",
    "        result = all(key in target for key in validation_keys)\n",
    "\n",
    "        if not result:\n",
    "            raise KeyError(\"target must contain 'boxes' and 'labels'\")\n",
    "\n",
    "        # Checking if the coordinates are in the xyxy format\n",
    "        boxes = target['boxes']\n",
    "        labels = target['labels']\n",
    "\n",
    "        boxes = tf.convert_to_tensor(boxes)\n",
    "        labels = tf.convert_to_tensor(labels)\n",
    "\n",
    "        if boxes.shape.rank == 1:\n",
    "            n = tf.shape(boxes)[0]\n",
    "\n",
    "            boxes = tf.cond(tf.equal(n,0), lambda: tf.reshape(boxes, [0,4]), lambda: tf.reshape(boxes, [1,4]))\n",
    "\n",
    "        if labels.shape.rank == 0:\n",
    "            labels = tf.reshape(labels,[1])      \n",
    "\n",
    "        # Checking and enforcing dtypes\n",
    "        boxes = tf.cast(boxes,tf.float32)\n",
    "        labels = tf.cast(labels, tf.int32)\n",
    "\n",
    "        target['boxes'] = boxes\n",
    "        target['labels'] = labels\n",
    "\n",
    "        # Making sure the metadata exists\n",
    "        if 'image_id' not in target:\n",
    "            target['image_id'] = tf.constant(f'{index}',dtype= tf.string)\n",
    "        else:\n",
    "            # Convert to tensor with the initial value\n",
    "            target['image_id'] = tf.constant(target['image_id'],dtype= tf.string)\n",
    "\n",
    "        if 'hash_signature' not in target:\n",
    "            target['hash_signature'] = tf.constant('',dtype= tf.string)\n",
    "        else:\n",
    "            target['hash_signature'] = tf.constant(target['hash_signature'],dtype= tf.string)\n",
    "\n",
    "        if 'orig_size' not in target:\n",
    "            target['orig_size'] = tf.shape(image)[0:2]\n",
    "\n",
    "        if 'path' not in target:\n",
    "            target['path'] = tf.constant(\"\", dtype= tf.string)\n",
    "\n",
    "        return target\n",
    "\n",
    "    def _sanitize_target(self, image: tf.Tensor, target : dict):\n",
    "        # Check for degenerate boxes which are x2 <= x1 or y2 <= y1\n",
    "        boxes = target['boxes']\n",
    "        labels = target['labels']\n",
    "\n",
    "        finite_mask = tf.reduce_all(tf.math.is_finite(boxes), axis=-1)\n",
    "        boxes = tf.boolean_mask(boxes, finite_mask)\n",
    "        labels = tf.boolean_mask(labels, finite_mask)\n",
    "\n",
    "        # Clip boxes to the original dimensions\n",
    "        H,W = target['orig_size']\n",
    "\n",
    "        H = tf.cast(H, tf.float32)\n",
    "        W = tf.cast(W, tf.float32)\n",
    "    \n",
    "        x1, y1, x2, y2 = tf.split(boxes,num_or_size_splits = 4, axis = -1)\n",
    "\n",
    "        x1 = tf.cast(x1, tf.float32)\n",
    "        y1 = tf.cast(y1, tf.float32)\n",
    "        x2 = tf.cast(x2, tf.float32)\n",
    "        y2 = tf.cast(y2, tf.float32)\n",
    "\n",
    "        x1 = tf.clip_by_value(x1, 0, W)\n",
    "        y1 = tf.clip_by_value(y1, 0, H)\n",
    "        x2 = tf.clip_by_value(x2, 0, W)\n",
    "        y2 = tf.clip_by_value(y2, 0, H)\n",
    "\n",
    "        boxes = tf.concat([x1, y1, x2, y2], axis = -1)\n",
    "\n",
    "        x1, y1, x2, y2 = tf.split(boxes, num_or_size_splits = 4, axis = -1)\n",
    "\n",
    "        degenerate_validity = tf.math.logical_or(x2 <= x1, y2 <= y1)\n",
    "        degenerate_validity = tf.reshape(tf.math.logical_not(degenerate_validity),[-1])\n",
    "\n",
    "        boxes = tf.boolean_mask(boxes,degenerate_validity)\n",
    "\n",
    "        # Filtering the labels too\n",
    "        labels = tf.boolean_mask(labels, degenerate_validity)\n",
    "\n",
    "        target['boxes'] = boxes\n",
    "        target['labels'] = labels\n",
    "\n",
    "        return target\n",
    "    \n",
    "\n",
    "    def _validate_target(self, image: tf.Tensor, target : dict):\n",
    "        # Validating the shape of the targets\n",
    "\n",
    "        boxes = target['boxes']\n",
    "        labels = target['labels']\n",
    "\n",
    "        # Checking for Target boxes\n",
    "        tf.debugging.assert_equal(tf.rank(boxes), 2, \"boxes must be rank-2: [N,4]\")\n",
    "        tf.debugging.assert_equal(tf.shape(boxes)[-1], 4, \"boxes last dim must be 4\")\n",
    "        tf.debugging.assert_equal(tf.rank(labels), 1, \"labels must be rank-1: [N]\")\n",
    "        tf.debugging.assert_equal(tf.shape(boxes)[0], tf.shape(labels)[0], \"boxes and labels must have same N\")\n",
    "        \n",
    "\n",
    "        tf.debugging.assert_equal(tf.reduce_all(tf.math.is_finite(boxes)), True, \"boxes contain NaN/Inf\")\n",
    "\n",
    "        x1, y1, x2, y2 = tf.split(boxes,num_or_size_splits = 4, axis = -1)\n",
    "\n",
    "        x1 = tf.cast(x1, tf.float32)\n",
    "        y1 = tf.cast(y1, tf.float32)\n",
    "        x2 = tf.cast(x2, tf.float32)\n",
    "        y2 = tf.cast(y2, tf.float32)\n",
    "\n",
    "        tf.debugging.assert_less_equal(x1, x2, message= \" x1 <= x2 condition violated\")\n",
    "        tf.debugging.assert_less_equal(y1, y2, message= \" y1 <= y2 condition violated\")\n",
    "\n",
    "        # Checking if the coordinates are in the bounds of the image\n",
    "        image_shape = tf.shape(image)\n",
    "    \n",
    "        H = tf.cast(image_shape[0], tf.float32)\n",
    "        W = tf.cast(image_shape[1], tf.float32)\n",
    "    \n",
    "        # Checking if the coordinates are within bounds\n",
    "        x1_condition = tf.math.logical_and((x1 >= 0),(x1 <= W))\n",
    "        x2_condition = tf.math.logical_and((x2 >= 0),(x2 <= W))\n",
    "    \n",
    "        x_validity = tf.reduce_all(tf.math.logical_and(x1_condition, x2_condition ))\n",
    "\n",
    "        y1_condition = tf.math.logical_and((y1 >= 0),(y1 <= H))\n",
    "        y2_condition = tf.math.logical_and((y2 >= 0),(y2 <= H))\n",
    "    \n",
    "        y_validity = tf.reduce_all(tf.math.logical_and(y1_condition, y2_condition))\n",
    "    \n",
    "        tf.debugging.assert_equal(x_validity, tf.constant(True, tf.bool), message = \"Failed to validate x conditions\")\n",
    "        tf.debugging.assert_equal(y_validity, tf.constant(True, tf.bool), message = \"Failed to validate y conditions\")\n",
    "\n",
    "        tf.debugging.assert_type(labels, tf.int32)\n",
    "        tf.debugging.assert_greater_equal(tf.reduce_min(labels), 1, \"labels must be >= 1 (0 is background)\")\n",
    "\n",
    "        # Checking for the other targets attributes\n",
    "        tf.debugging.assert_equal(tf.rank(target[\"image_id\"]), 0, \"image_id must be scalar\")\n",
    "        tf.debugging.assert_equal(tf.rank(target[\"orig_size\"]), 1, \"orig_size must be rank-1\")\n",
    "        tf.debugging.assert_equal(tf.shape(target[\"orig_size\"])[0], 2, \"orig_size must be [2] (H,W)\")\n",
    "        tf.debugging.assert_less_equal(tf.reduce_max(labels),self._num_classes - 1,\"label id out of range\")\n",
    "\n",
    "        # Checking the intensity values of an image\n",
    "        tf.debugging.assert_equal(tf.rank(image), 3, \"image must be [H,W,3]\")\n",
    "        tf.debugging.assert_equal(tf.shape(image)[-1], 3, \"image must have 3 channels\")\n",
    "\n",
    "        tf.debugging.assert_equal(tf.rank(image),tf.constant(3, dtype = tf.int32), message = \"The rank is not the same for the images\")\n",
    "        tf.debugging.assert_equal(tf.shape(image)[-1],tf.constant(3, dtype = tf.int32), message = \"The channel dimension is invalid for image\")\n",
    "        tf.debugging.assert_equal(tf.math.reduce_all(tf.math.is_finite(tf.cast(image, dtype=tf.float32))), tf.constant(True, dtype= tf.bool), message = \"The image intensities are not finite\")\n",
    "            \n",
    "        tf.debugging.assert_equal(target['path'].dtype, tf.string, \"path is invalid for targets\") \n",
    "    \n",
    "    @abstractmethod\n",
    "    def get_image_id(self, index: int):\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def __getitem__(self, index: int):\n",
    "        image, target = self._load_raw_sample(index)\n",
    "\n",
    "        target = self._standardize_target(image, target, index)\n",
    "\n",
    "        target = self._sanitize_target(image, target)\n",
    "\n",
    "        self._validate_target(image, target)\n",
    "\n",
    "        if self._transform is not None:\n",
    "            image, target = self._transform(image, target)\n",
    "            self._validate_target(image, target)\n",
    "\n",
    "        return image, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8ae6322b-e25a-4cc7-951e-c58cbfe4e253",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _standardize_target(image, target : dict, index):\n",
    "    # Checking for keys in the targets\n",
    "    validation_keys = ['boxes','labels']\n",
    "    result = all(key in target for key in validation_keys)\n",
    "\n",
    "    if not result:\n",
    "        raise IndexError(\"Standardization failed for images\")\n",
    "\n",
    "    # Checking if the coordinates are in the xyxy format\n",
    "    boxes = target['boxes']\n",
    "    labels = target['labels']\n",
    "\n",
    "    boxes = tf.convert_to_tensor(boxes)\n",
    "    labels = tf.convert_to_tensor(labels)\n",
    "\n",
    "    if boxes.shape.rank == 1:\n",
    "        n = tf.shape(boxes)[0]\n",
    "\n",
    "        boxes = tf.cond(tf.equal(n,0), lambda: tf.reshape(boxes, [0,4]), lambda: tf.reshape(boxes, [1,4]))\n",
    "\n",
    "    if labels.shape.rank == 0:\n",
    "        labels = tf.reshape(labels,[1])      \n",
    "\n",
    "    # Checking and enforcing dtypes\n",
    "    boxes = tf.cast(boxes,tf.float32)\n",
    "    labels = tf.cast(labels, tf.int32)\n",
    "\n",
    "    target['boxes'] = boxes\n",
    "    target['labels'] = labels\n",
    "\n",
    "    # Making sure the metadata exists\n",
    "    if 'image_id' not in target:\n",
    "        target['image_id'] = index\n",
    "\n",
    "    if 'orig_size' not in target:\n",
    "        target['orig_size'] = tf.shape(image)[0:2]\n",
    "\n",
    "    if 'path' not in target:\n",
    "        target['path'] = ''\n",
    "\n",
    "    return target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9bfd7da5-df06-42c3-b8e4-0f3fc4dd1057",
   "metadata": {},
   "outputs": [],
   "source": [
    "image = tf.zeros([480, 640, 3], dtype=tf.uint8)\n",
    "\n",
    "target = {\n",
    "    \"boxes\": tf.constant([\n",
    "        [ 50.0,  60.0, 200.0, 300.0],\n",
    "        [320.0, 100.0, 500.0, 400.0],\n",
    "    ], dtype=tf.float32),\n",
    "    \"labels\": tf.constant([7, 15], dtype=tf.int32),\n",
    "    \"image_id\": \"2007_000027\",\n",
    "    \"path\": \"/datasets/VOCdevkit/VOC2007/JPEGImages/2007_000027.jpg\",\n",
    "}\n",
    "idx = 27"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "14c120a1-10b5-4c11-9317-2199381e876b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'boxes': <tf.Tensor: shape=(2, 4), dtype=float32, numpy=\n",
       " array([[ 50.,  60., 200., 300.],\n",
       "        [320., 100., 500., 400.]], dtype=float32)>,\n",
       " 'labels': <tf.Tensor: shape=(2,), dtype=int32, numpy=array([ 7, 15], dtype=int32)>,\n",
       " 'image_id': '2007_000027',\n",
       " 'path': '/datasets/VOCdevkit/VOC2007/JPEGImages/2007_000027.jpg',\n",
       " 'orig_size': <tf.Tensor: shape=(2,), dtype=int32, numpy=array([480, 640], dtype=int32)>}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_standardize_target(image, target, idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "25117233-1ee0-4b01-855e-b82abc9117eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "image = tf.zeros([375, 500, 3], dtype=tf.uint8)\n",
    "\n",
    "target = {\n",
    "    \"boxes\": [\n",
    "        [48, 240, 195, 371],   # ints are fine as input\n",
    "        [  8,  12, 352, 150],\n",
    "    ],\n",
    "    \"labels\": [1, 14],         # should become int32 tensor\n",
    "}\n",
    "idx = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e5decea3-bac3-45d6-b134-30dd2e9463c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'boxes': <tf.Tensor: shape=(2, 4), dtype=float32, numpy=\n",
       " array([[ 48., 240., 195., 371.],\n",
       "        [  8.,  12., 352., 150.]], dtype=float32)>,\n",
       " 'labels': <tf.Tensor: shape=(2,), dtype=int32, numpy=array([ 1, 14], dtype=int32)>,\n",
       " 'image_id': 3,\n",
       " 'orig_size': <tf.Tensor: shape=(2,), dtype=int32, numpy=array([375, 500], dtype=int32)>,\n",
       " 'path': ''}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_standardize_target(image, target, idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c2ff006b-94e1-45f2-9661-d336004e783c",
   "metadata": {},
   "outputs": [],
   "source": [
    "image = tf.zeros([224, 224, 3], dtype=tf.uint8)\n",
    "\n",
    "target = {\n",
    "    \"boxes\": [20, 30, 100, 180],  # WRONG SHAPE INPUT (should be [[...]])\n",
    "    \"labels\": 5,                  # scalar label\n",
    "}\n",
    "idx = 99"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "96c1b072-19c3-48f3-961f-df1b438761cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'boxes': <tf.Tensor: shape=(1, 4), dtype=float32, numpy=array([[ 20.,  30., 100., 180.]], dtype=float32)>,\n",
       " 'labels': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([5], dtype=int32)>,\n",
       " 'image_id': 99,\n",
       " 'orig_size': <tf.Tensor: shape=(2,), dtype=int32, numpy=array([224, 224], dtype=int32)>,\n",
       " 'path': ''}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_standardize_target(image, target, idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9b0cc86d-613e-438a-93b3-5269742edf4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _sanitize_target(image, target):\n",
    "    # Check for degenerate boxes which are x2 <= x1 or y2 <= y1\n",
    "    boxes = target['boxes']\n",
    "    labels = target['labels']\n",
    "\n",
    "    finite_mask = tf.reduce_all(tf.math.is_finite(boxes), axis=-1)\n",
    "    boxes = tf.boolean_mask(boxes, finite_mask)\n",
    "    labels = tf.boolean_mask(labels, finite_mask)\n",
    "\n",
    "    if tf.shape(boxes)[0] == 0:\n",
    "        target[\"boxes\"] = boxes\n",
    "        target[\"labels\"] = labels\n",
    "        return target\n",
    "\n",
    "    # Clip boxes to the original dimensions\n",
    "    H,W = target['orig_size']\n",
    "\n",
    "    H = tf.cast(H, tf.float32)\n",
    "    W = tf.cast(W, tf.float32)\n",
    "    \n",
    "    x1, y1, x2, y2 = tf.split(boxes,num_or_size_splits = 4, axis = -1)\n",
    "\n",
    "    x1 = tf.cast(x1, tf.float32)\n",
    "    y1 = tf.cast(y1, tf.float32)\n",
    "    x2 = tf.cast(x2, tf.float32)\n",
    "    y2 = tf.cast(y2, tf.float32)\n",
    "\n",
    "    x1 = tf.clip_by_value(x1, 0, W)\n",
    "    y1 = tf.clip_by_value(y1, 0, H)\n",
    "    x2 = tf.clip_by_value(x2, 0, W)\n",
    "    y2 = tf.clip_by_value(y2, 0, H)\n",
    "\n",
    "    boxes = tf.concat([x1, y1, x2, y2], axis = -1)\n",
    "\n",
    "    x1, y1, x2, y2 = tf.split(boxes, num_or_size_splits = 4, axis = -1)\n",
    "\n",
    "    degenerate_validity = tf.math.logical_or(x2 <= x1, y2 <= y1)\n",
    "    degenerate_validity = tf.reshape(tf.math.logical_not(degenerate_validity),[-1])\n",
    "\n",
    "    boxes = tf.boolean_mask(boxes,degenerate_validity)\n",
    "\n",
    "    # Filtering the labels too\n",
    "    labels = tf.boolean_mask(labels, degenerate_validity)\n",
    "\n",
    "    target['boxes'] = boxes\n",
    "    target['labels'] = labels\n",
    "\n",
    "    return target\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b3d59065-b4c6-4913-882e-e4d9f9774702",
   "metadata": {},
   "outputs": [],
   "source": [
    "image = tf.random.uniform(shape=(224, 224, 3), minval=0, maxval=255, dtype=tf.int32)\n",
    "image = tf.cast(image, tf.uint8)  # typical \"raw\" image dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "29df17dd-b81b-4bc2-91e5-ad851ab41e3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_clean = {\n",
    "    \"boxes\": tf.constant([\n",
    "        [10.0,  20.0,  60.0,  80.0],   # valid\n",
    "        [120.0, 40.0, 200.0, 210.0],   # valid\n",
    "    ], dtype=tf.float32),\n",
    "    \"labels\": tf.constant([3, 15], dtype=tf.int32),\n",
    "    \"image_id\": tf.constant(42),\n",
    "    \"path\": \"VOCdevkit/VOC2007/JPEGImages/000042.jpg\",\n",
    "    \"orig_size\": tf.constant([224, 224], dtype=tf.int32),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7d0e6122-33d4-458a-8538-7a726576556d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'boxes': <tf.Tensor: shape=(2, 4), dtype=float32, numpy=\n",
       " array([[ 10.,  20.,  60.,  80.],\n",
       "        [120.,  40., 200., 210.]], dtype=float32)>,\n",
       " 'labels': <tf.Tensor: shape=(2,), dtype=int32, numpy=array([ 3, 15], dtype=int32)>,\n",
       " 'image_id': <tf.Tensor: shape=(), dtype=int32, numpy=42>,\n",
       " 'path': 'VOCdevkit/VOC2007/JPEGImages/000042.jpg',\n",
       " 'orig_size': <tf.Tensor: shape=(2,), dtype=int32, numpy=array([224, 224], dtype=int32)>}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_sanitize_target(image, target_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "162b2760-488c-400c-a968-47112d99c45e",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_degenerate = {\n",
    "    \"boxes\": tf.constant([\n",
    "        [10.0,  20.0,  60.0,  80.0],   # valid\n",
    "        [50.0,  30.0,  40.0,  90.0],   # x2 < x1  -> invalid\n",
    "        [12.0, 100.0,  40.0,  90.0],   # y2 < y1  -> invalid\n",
    "        [70.0,  70.0,  70.0, 120.0],   # x2 == x1 -> zero width -> invalid\n",
    "    ], dtype=tf.float32),\n",
    "    \"labels\": tf.constant([1, 2, 3, 4], dtype=tf.int32),\n",
    "    \"image_id\": tf.constant(7),\n",
    "    \"orig_size\": tf.constant([224, 224], dtype=tf.int32),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a0d0b85d-6fde-48b2-8518-eb3169262a05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'boxes': <tf.Tensor: shape=(1, 4), dtype=float32, numpy=array([[10., 20., 60., 80.]], dtype=float32)>,\n",
       " 'labels': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>,\n",
       " 'image_id': <tf.Tensor: shape=(), dtype=int32, numpy=7>,\n",
       " 'orig_size': <tf.Tensor: shape=(2,), dtype=int32, numpy=array([224, 224], dtype=int32)>}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_sanitize_target(image, target_degenerate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "eae8db6d-9170-4ac1-af33-4b320250cbf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_oob = {\n",
    "    \"boxes\": tf.constant([\n",
    "        [-10.0,  10.0,  50.0,  60.0],  # x1 < 0\n",
    "        [ 20.0, -15.0, 100.0,  40.0],  # y1 < 0\n",
    "        [150.0, 150.0, 300.0, 260.0],  # x2, y2 > bounds\n",
    "        [-20.0, -20.0, 500.0, 500.0],  # huge overflow\n",
    "    ], dtype=tf.float32),\n",
    "    \"labels\": tf.constant([5, 6, 7, 8], dtype=tf.int32),\n",
    "    \"image_id\": tf.constant(99),\n",
    "    \"orig_size\": tf.constant([224, 224], dtype=tf.int32),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8469b25c-c166-40d7-8e3e-bbfe5da50846",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'boxes': <tf.Tensor: shape=(4, 4), dtype=float32, numpy=\n",
       " array([[  0.,  10.,  50.,  60.],\n",
       "        [ 20.,   0., 100.,  40.],\n",
       "        [150., 150., 224., 224.],\n",
       "        [  0.,   0., 224., 224.]], dtype=float32)>,\n",
       " 'labels': <tf.Tensor: shape=(4,), dtype=int32, numpy=array([5, 6, 7, 8], dtype=int32)>,\n",
       " 'image_id': <tf.Tensor: shape=(), dtype=int32, numpy=99>,\n",
       " 'orig_size': <tf.Tensor: shape=(2,), dtype=int32, numpy=array([224, 224], dtype=int32)>}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_sanitize_target(image, target_oob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e778ac95-79f7-4b56-9e26-3e728255df1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_nan_inf = {\n",
    "    \"boxes\": tf.constant([\n",
    "        [10.0, 20.0, 60.0, 80.0],            # valid\n",
    "        [float(\"nan\"), 5.0, 20.0, 25.0],     # NaN\n",
    "        [10.0, float(\"inf\"), 20.0, 25.0],    # Inf\n",
    "    ], dtype=tf.float32),\n",
    "    \"labels\": tf.constant([1, 2, 3], dtype=tf.int32),\n",
    "    \"image_id\": tf.constant(555),\n",
    "    \"orig_size\": tf.constant([224, 224], dtype=tf.int32),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e3951b24-7548-4f87-adf5-ed001c847978",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'boxes': <tf.Tensor: shape=(1, 4), dtype=float32, numpy=array([[10., 20., 60., 80.]], dtype=float32)>,\n",
       " 'labels': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>,\n",
       " 'image_id': <tf.Tensor: shape=(), dtype=int32, numpy=555>,\n",
       " 'orig_size': <tf.Tensor: shape=(2,), dtype=int32, numpy=array([224, 224], dtype=int32)>}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_sanitize_target(image, target_nan_inf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "be2c5d03-0100-4fca-8468-f8408b72e250",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _validate_target(image, target):\n",
    "    # Validating the shape of the targets\n",
    "\n",
    "    boxes = target['boxes']\n",
    "    labels = target['labels']\n",
    "\n",
    "    # Checking for Target boxes\n",
    "    tf.debugging.assert_equal(tf.rank(boxes), 2, \"boxes must be rank-2: [N,4]\")\n",
    "    tf.debugging.assert_equal(tf.shape(boxes)[-1], 4, \"boxes last dim must be 4\")\n",
    "    tf.debugging.assert_equal(tf.rank(labels), 1, \"labels must be rank-1: [N]\")\n",
    "    tf.debugging.assert_equal(tf.shape(boxes)[0], tf.shape(labels)[0], \"boxes and labels must have same N\")\n",
    "\n",
    "    tf.debugging.assert_equal(tf.reduce_all(tf.math.is_finite(boxes)), True, \"boxes contain NaN/Inf\")\n",
    "\n",
    "    x1, y1, x2, y2 = tf.split(boxes,num_or_size_splits = 4, axis = -1)\n",
    "\n",
    "    x1 = tf.cast(x1, tf.float32)\n",
    "    y1 = tf.cast(y1, tf.float32)\n",
    "    x2 = tf.cast(x2, tf.float32)\n",
    "    y2 = tf.cast(y2, tf.float32)\n",
    "\n",
    "    tf.debugging.assert_less_equal(x1, x2, message= \" x1 <= x2 condition violated\")\n",
    "    tf.debugging.assert_less_equal(y1, y2, message= \" y1 <= y2 condition violated\")\n",
    "\n",
    "    # Checking if the coordinates are in the bounds of the image\n",
    "    H, W, _ = tf.shape(image)\n",
    "    \n",
    "    H = tf.cast(H, tf.float32)\n",
    "    W = tf.cast(W, tf.float32)\n",
    "    \n",
    "    # Checking if the coordinates are within bounds\n",
    "    x1_condition = tf.math.logical_and((x1 >= 0),(x1 <= W))\n",
    "    x2_condition = tf.math.logical_and((x2 >= 0),(x2 <= W))\n",
    "    \n",
    "    x_validity = tf.reduce_all(tf.math.logical_and(x1_condition, x2_condition ))\n",
    "\n",
    "    y1_condition = tf.math.logical_and((y1 >= 0),(y1 <= H))\n",
    "    y2_condition = tf.math.logical_and((y2 >= 0),(y2 <= H))\n",
    "    \n",
    "    y_validity = tf.reduce_all(tf.math.logical_and(y1_condition, y2_condition))\n",
    "    \n",
    "    tf.debugging.assert_equal(x_validity, tf.constant(True, tf.bool), message = \"Failed to validate x conditions\")\n",
    "    tf.debugging.assert_equal(y_validity, tf.constant(True, tf.bool), message = \"Failed to validate y conditions\")\n",
    "\n",
    "    tf.debugging.assert_type(labels, tf.int32)\n",
    "    tf.debugging.assert_greater_equal(tf.reduce_min(labels), 1, \"labels must be >= 1 (0 is background)\")\n",
    "\n",
    "    # Checking for the other targets attributes\n",
    "    tf.debugging.assert_equal(tf.rank(target[\"image_id\"]), 0, \"image_id must be scalar\")\n",
    "    tf.debugging.assert_equal(tf.rank(target[\"orig_size\"]), 1, \"orig_size must be rank-1\")\n",
    "    tf.debugging.assert_equal(tf.shape(target[\"orig_size\"])[0], 2, \"orig_size must be [2] (H,W)\")\n",
    "\n",
    "    # Checking the intensity values of an image\n",
    "    tf.debugging.assert_equal(tf.rank(image), 3, \"image must be [H,W,3]\")\n",
    "    tf.debugging.assert_equal(tf.shape(image)[-1], 3, \"image must have 3 channels\")\n",
    "\n",
    "    tf.debugging.assert_equal(tf.rank(image),tf.constant(3, dtype = tf.int32), message = \"The rank is not the same for the images\")\n",
    "    tf.debugging.assert_equal(tf.shape(image)[-1],tf.constant(3, dtype = tf.int32), message = \"The channel dimension is invalid for image\")\n",
    "    tf.debugging.assert_equal(tf.math.reduce_any(tf.math.is_finite(tf.cast(image, dtype=tf.float32))), tf.constant(True, dtype= tf.bool), message = \"The image intensities are not finite\")\n",
    "     \n",
    "    intensity_invalid = tf.reduce_any(tf.logical_or(image < 0, image > 255))\n",
    "    tf.debugging.assert_equal(intensity_invalid, False, \"image values must be in [0,255]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c1dda567-b1a0-4070-8d14-275cc456de9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "_validate_target(image,target_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64285dde-5c3e-4fec-b9ff-06f75a8afab2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
